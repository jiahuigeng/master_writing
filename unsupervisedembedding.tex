\chapter{Unsupervised Learning of Cross-lingual Word Embedding}
%Proposed cross-lingual embedding learning requires several thousands of word pairs as anchors to learn the mapping with good generalization ability to predict for unseen word. But as for low-source language, dictionary of good quality is not readily available. This motivates more and more works on unsupervised cross-lingual word embedding learning, if the unsupervised method works, the word-level or grammar-level properties will be better captured, since it enables the knowledge transfer for those less-studied languages.

Approaches in the Section 3.2 is to learn the shared embedding space for words across different languages. They all must employ parallel data like a small bilingual dictionary or weak supervision signals like parallel sentences. The transformation generalizes well to the whole vocabulary. Even for unseen words, we can also infer word translation of good quality. Starting from small lexicon we are able to extend the lexicon. However labeling the lexicon information manually still costs lots of labor. This motivates people to bootstrap from few seed entries. The idea behind the bootstrapping approach is to find heuristics like cognates, numbers as initialization, then iteratively expand the seed dictionary. In this chapter, some good initialization approaches will be introduced. Then the iterative training principle will be introduced. Finally I explain a novel data-driven training approach in detail.

%\begin{figure}[t]
%	\includegraphics[width=12cm]{iter1}
%	\centering
%	\caption{Accuracy on bilingual lexicon induction}
%\end{figure}	
%
%
%
%
%
%
%
\section{Initialization}
 \cite{vulic2016role} shows that a seed dictionary of at least hundreds are need for the model to be general enough. \cite{artetxe2017learning} raise a mapping model that relies only on a small set of shared words (e.g., identically spelled words or only shared numbers) as seed for the procedure. But this approach is very dependent on the writing system, only works for specific language pairs. Also it gets stuck in poor local optima when the initial seed cannot provide enough good information to start the system.\\
  \cite{DBLP:journals/corr/abs-1801-06126}
 propose an approach which first use PCA to realize an approximate alignment, then use mini-batch cycle iterative closest point to learn the mapping.  
\cite{zhang2017adversarial} put forward an adversarial autoencoder, using the generator $G$ to implement the mapping so that the transformed source embedding similar to the corresponding target embedding, and the discriminator $D$ strives to distinguish the target embedding from the transformed source embedding. \cite{conneau2017word} simplifies the structure and proposed an unsupervised criterion that is highly correlated with the quality of the mapping which can be used as stop criterion and to select the best hyper-parameters.
%
%As it can be clearly observed from the figure below that, the self learning algorithm can effectively learn the mapping between the source and target distribution even starts with a small dictionary size and the final accuracy when the algorithm converges are nearly the same level. However when start randomly without any support of dictionary, the framework does not work. In order to realize a fully unsupervised learning, we need to design heuristics to build the seed dictionary.
%
%For total unsupervised learning, there is no direct alignment between the languages. 	The most challenge for unsupervised learning is the initialization. As proved in the experiment of \cite{}, the self-learning framework can work even with a small dictionary set, however the model cannot work without such dictionary as hint.  So it is important to find heuristic methods to initialize the model.
%
%Several models are proposed:
%Based on matrices ${M_E = EE^T}$, ${M_F = FF^T}$, we can exploit latent information to reduce the mismatch. Assume that, the words in the source and target set are most common. For a specific word, the corresponding line in ${M_E}$ demonstrates the distribution of similarity in the source vocabulary. ${M_E}$, ${M_F}$ can be nearly equivalent up to a permutation of the words. 
%Instead of explore all the probability of permutation, he first sorts the values in each row of ${M_F}$ and ${M_E}$
%
%Assume the SVD of ${E = USV^T}$,  the similarity ${M_E = US^2U^T}$. In practice, he computed sorted ${1}$, ${2}$	yielding the matrix ${E^{\prime}}$ , ${F^{\prime}}$ and used to build the initial solution for self-learning.


\subsection{Adversarial training}
Generative adversarial networks are a class of artificial intelligence algorithm used in unsupervised learning. The objective of generative network (generator) is to increase the error rate of the discriminative network (discriminator) while the discriminator discriminates between instances from the true data distribution and candidates produced by the generator. 
Following this principle, the generator tries to learning to mapping from the source embedding space to target embedding space, so that the discriminator while could be a deep neural network cannot distinguish the data source. \\
Let ${=\{ \bm{f}_1, \cdots, \bm{f}_n\}}$ and ${ = \{ \bm{e}_i, \cdots , \bm{e}_m\}}$ be the two sets of $n$ and $m$ word embeddings from a source and a target language separately, We refer the discriminator parameters as ${\theta_D}$.The discriminator is a multi-layer neural network trained to discriminate the transformed source word embedding from the target word embedding, while the mapping $W$, simply a linear transformation, is trained to fooling discriminator. In the two-player game, we are supposed to learn the mapping from source embedding space to the target space.

In the first adversarial training method for cross-lingual word embedding, the objective of the two networks are as followed:\\
Discriminator objective  
\[ L_D(\theta_D | W) =  -\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_D}(source = 1| W\bm{f}_i) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_D}(source=0| \bm{e}_i) \]	
Generator objective 
\[ L_W(W|\theta_D) =  -\frac{1}{n} \sum_{i=1}^{n}\log P_{\theta_D}(source=0|W \bm{f}_i)  \]

To relax the orthogonal constraint, the authors introduce the adversarial autoencoder, after the mapping $W $from source embedding space to target embedding space, the source embedding should also be mapping back with $W^T$, therefore, they introduce the reconstruction loss as 
\[ L_W(W|\theta_D) =  -\frac{1}{n} \sum_{i=1}^{n} \{\log P_{\theta_D}(source=0|W \bm{f}_i) -  \lambda \cos (\bm{f}_i, W^T W \bm{f}_i)\}\]

In \cite{conneau2017word} work, they just take a symmetric loss for the generator loss and achieve better results:
\[ L_W(W|\theta_D) =  -\frac{1}{n} \sum_{i=1}^{n}\log P_{\theta_D}(source=0|W \bm{f}_i) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_D}(source = 1 | \bm{e}_i) \]





\subsubsection{Model Selection}
Since the cross-lingual embedding training is under the unsupervised setting, We do not know the word translation accuracy, otherwise if we have the validation data, that means we will have parallel data, against the unsupervised idea. To address this issue, we must select from the property of data or the loss of the neural network as the unsupervised criterion. However in the experiments we find that the accuracy of the discriminator always stays at a high level no matter how is the word translation accuracy. 

All these methods can be use to find meaningful word pairs in both languages. For further refinement we can use the induced dictionary to start the iterative self-learning algorithm. 

\subsection{Iterative Closest Point Method}
The method assumes that many language pairs share some principle axes of variation. For each language, first select most frequent word vector,center the data and project it to the top ${p}$ principle components.
%Propose a method first learn ${T_{ef}}$ and ${T_{fe}}$ for ${E \rightarrow F}$ and ${F \rightarrow E}$ , we include the cycle-constraints ensure that a word ${f}$ transformed into joint embedding space and transformed back is unchanged. 
The normal Iterative Closest Point (ICP) is unidirectional, the Mini-batch Cycle ICP (MBC-ICP) includes cycle-constrains ensuring that a word transformed to another language could also be transformed back and stay unchanged. Each iteration the final MBC-ICP algorithm can describled as: 

\begin{enumerate}
	\item For each target word ${\bm{e}}$, find the nearest ${W_{f\rightarrow e} \bm{f(e)}}$, denote source word $\bm{f(e)}$
	\item For each source word ${\bm{f}}$, find the nearest ${W_{e\rightarrow f} \bm{e(f)}}$, denote source word $\bm{e(f)}$
	\item Optimize ${W_{e\rightarrow f}}$ and ${W_{f\rightarrow e}}$: 
	\begin{align*}
	\mathcal{L} = &  \sum_{\bm{e}} {\lVert \bm{e} - W_{f\rightarrow e} \bm{f(e)}\rVert} + \sum_{\bm{f}} {\rVert \bm{f} - W_{e \rightarrow f} \bm{e(f)} \rVert} \\
	& + \lambda \sum_{\bm{e}}{\lVert \bm{e} - W_{f\rightarrow e} W_{e\rightarrow f} \bm{e} \rVert} + \lambda {\sum_{\bm{f}}} {\lVert \bm{f} - W_{e\rightarrow f} W_{f \rightarrow e} \bm{f} \rVert}
	\end{align*}
	
\end{enumerate}
\section{Iterative Training}
\begin{figure}[h]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H]
			\SetAlgoLined
			\KwIn{$\mathcal{F}$ (source embeddings)}
			\KwIn{$\mathcal{E}$ (target embeddings)}
			\KwIn{$\mathcal{D}$ (seed dictionary)}
			\KwResult{$\mathcal{W}$ (embedding mapping) }
			\While{not converge}{
				${\mathcal{W} \leftarrow LEARN\_MAPPING(\mathcal{F},\mathcal{E},\mathcal{D})}$
				${\mathcal{D} \leftarrow LEARN\_DICTIONARY}$
				
			}
			
			\caption{Iterative training procedure}
		\end{algorithm}
	\end{minipage}
\end{figure}
Assume that with the initialization step we can already get roughly aligned distribution of the source and target embedding, and we can use this mapping to learn a good dictionary (word pairs). Since the quality of dictionary gets improved, we can further learn a better mapping, consequently, such process can be repeated iteratively until convergence criterion is met.
\subsection{Self-learning with Procrustes}
For self-learning frame work, we compute the optimal dictionary based on the mapped source word embeddings and target word embeddings. To sure the quality of the small dictionary, we choose only top frequent words for both languages and only bidirectional translations are kept. We even filter the dictionary with some threshold.


\subsection{Data-driven Method}
Since we have large monolingual data and one of the tasks to evaluate the cross-lingual word embedding is build machhine translation system based on these cross-lingual embeddings. We make use of the language model and improve also the translation quality simultaneously. We can set the source from the translation, build the dictionary from the word translation pairs, in detail the training procedure are as followed:
\begin{enumerate}
	\item translate corpus according to current mapping, get the word pairs $D$
	\item train the network with $D$ to minimize the mapping distance
	\item Repeat $1,2$ until the algorithm converges
\end{enumerate}
		\[ 
		\left. \begin{array}{c c} 
		(f_1,& e_1)\\
		(f_2,& e_2)\\
		\multicolumn{2}{c}{\vdots}\\
		(f_N,& e_N)
		\end{array} \right\} 
		\Rightarrow D
		\]
Previously, the objective we want to minimize is the mean square error of the mapped embeddings. By training the mapping with neural network we can define different loss functions according to specific features
		\[\mathcal{L} = \sum_{(f,e)\in D} {\lVert Wf - e \rVert}^2  \]


\textbf{Translation}\\
When inferring word translation, we use nearest neighbor search to find the candidates, where we face the hubness problem. 
Points are tending to be nearest neighbors of many points in high-dimensional space.  Those points (hubs) will harm the search accuracy.\\

There are two approaches to handle this issue.
\begin{itemize}
	\item Inverted Softmax\\
	The confidence of choosing a target word as translation of a source word can be considered as softmax-like normalized probability
	\[ p(e|f) = \frac{\exp{\Big(\beta \cdot  \text{score}(e,f)\Big)}}{\sum_{e^{\prime}} {\exp{\Big(\beta \cdot \text{score}(e^{\prime}, f)\Big)}}} \]
	where the $\text{score}(\cdot)$ is the score function we can define ourselves.
	we learn the "temperature" $\beta$ by maximizing the log probability over the training dictionary. 
	\[ \argmax{\beta} \sum_{e,f} \ln p(e|f)  \]
	
	The author observed that, if we invert the softmax and normalizing the probability over all the source words rather than target words, the hubness problem could be mitigated:
	\[ p(e|f) = \frac{\exp{\Big(\beta \cdot  \text{score}(e,f)\Big)}}{\alpha \  \sum_{f^{\prime}} {\exp{\Big(\beta \cdot \text{score}(e, f^{\prime})\Big)}}}\]
	\item  Cross-domain Similarity Local Scaling (CSLS)\\
	We denote ${\mathcal{N}_e(\bm{f})}$ the set of ${k}$ nearest neighbors of points of the mapped $\bm{f}$ in the target embedding space, and ${\mathcal{N}_f(\bm{e})}$ the nearest neighbors of mapped ${\bm{e}}$ in the source embedding space. We consider the mean cosine similarity as hub-ness:
	\[ r(\bm{f})= \frac{1}{K} \sum_{\bm{e}^{\prime} \in \mathcal{N}_e(\bm{f})} \cos(\bm{e}^{\prime}, W\bm{f})\]
	So in this way we penalize the hub points:
	\[ CSLS(\bm{e}, \bm{f}) = 2 \cos(\bm{e}, \bm{f}) - \frac{1}{k} \sum_{\bm{e^{\prime}} \in \mathcal{N}_e(\bm{f})} \cos(\bm{f}, \bm{e^{\prime}})- \frac{1}{k} \sum_{\bm{f^{\prime}} \in \mathcal{N}_f(\bm{e})} cos(\bm{f^{\prime}}, \bm{e}) \]
\end{itemize}


Since we use the pseudo parallel data (source sentence and word-by-word target translation sentence), we can exploit the context information from language model to select the word translation candidate. Beam search with language model is implemented here. More details about sentence translation will be explained next chapter.\\


\textbf{Training}\\
It's common to set the training objective of neural mapping as the squared error of the Euclidean distance. This is also exact the same objective of Procrustes analysis, when seed dictionary is available. However, in some papers, cosine similarity loss is suggested since it is used for monolingual word embedding training. It's would be better to keep consistent.  \\
As mentioned previously, with orthogonal constraint, the problem can be viewed as an orthogonal Procrustes problem, which has a closed form solution and can be computed in linear time related to the vocabulary size. The orthogonality ensures the monolingual quality of the monolingual embeddings is preserved. In preserves the dot product of vectors and the $l_2$ distance.  In our experiments, we find the orthogonal constraint make the training procedure converge successfully, especially when at the beginning stage.
In this work, the orthogonal constraint is added during the training process. In detail, we use projected gradient descent to training the neural network, alternating the training and the orthogonal constraining by:
\[ W \leftarrow (1+\beta) W - \beta(WW^\top)W\]

\textbf{Evaluation}\\
In this thesis, the metrics of the cross-lingual word embedding is to retrieve the translation of given source words. In detail, by checking if the nearest neighbors of the source word are also in the candidates in a open word translation dictionary dataset. We report the word translation accuracy measured on top-1 prediction for word pairs with 1500 distinct source words.\\

