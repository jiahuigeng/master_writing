
\chapter{Introduction}
Building a good machine translation (MT) system requires an enormous collection of parallel data. Neural machine translation (NMT) systems has emerged as the
most promising machine translation approach in recent years, showing superior performance on public benchmarks and rapid adoption in deployments. However MT systems often fail when the training data is not enough. However, parallel corpora are expensive because they require huge human labor, time and expertise of corresponding languages.
\indent Several approaches has been proposed to alleviate this issue, for instance, triangulation or semi-supervised learning techniques; these systems however still need a strong cross-lingual signal. Unsupervised MT uses only monolingual data of the source and target languages to train the model and in fact, monolingual corpora are readily available.\\
Recently, cross-lingual word embedding draws more and more attention since it helps to explore the multilingual semantic information in a shared space. For instance, it enables computation of cross-lingual word similarities, which  can be directly applied to bilingual dictionary induction or cross-lingual information retrieval. Several methods are proposed to learn the cross-lingual cross-lingual word embedding without any parallel data. This correspondingly inspires work on unsupervised MT.\\
In this thesis, I propose a novel corpus-based unsupervised training method, the experiments the method can achieve competitive results  in comparison with the start-of-the-art methods. I further explore the 
application of the cross-lingual word embeddings in the MT task.
I develop a fully unsupervised MT system starting from the simple word-by-word translation principle. Language model (LM) is integrated to improve the translation. In order to boost the translation efficiency, I design a context-aware beam search to find the best translation candidate. To handle to reordering, I implement a denoising network with artificial noises, which are to mimic the true noises in the word-by-word translation, The results demonstrate that such simple but efficient translation system performs better even than most unsupervised neural MT system with costly iterative training.


\section{Related Work}

\subsection{Word Embedding}
\indent Traditional language processing systems treat words as discrete atomic symbols, which assign for each word a specific ID number. Such encodings are arbitrary; it does not provide any information about the relations that may exist between individual symbols. In comparison, distributed representations of words in a high dimensional continuous space help learning algorithm to achieve better performance in natural language processing. According to the distributional hypothesis, similar words tend to occur with similar neighbors, and have similar word representations. \\
\cite{mikolov2013efficient} propose two models for learning word embeddings with neural networks, namely skip-gram and continuous bag of words (CBOW) models. Neural architectures makes the training extremely efficient. In further work, \cite{mikolov2013distributed} discuss about learning the embedding for phrases. \cite{pennington2014glove} propose a regression model which combines the global matrix factorization and local context window methods, making the training process more interpretable. \cite{DBLP:journals/corr/BojanowskiGJM16} represent each word as a bag of character $n$-grams, and assign for each character $n$-gram a distinct embedding. In this way, they can exploit subword information. \\
\cite{mikolov2013exploiting} notice that continuous embedding spaces exhibit similar structures across languages, even for distant language pairs, for example, English-Vietnamese.  With separately learned monolingual embeddings they learn a linear mapping from source embedding space to target embedding space. To do this, they employ a parallel vocabulary of five thousand words as anchor points and evaluate their approach on a word translation task. \cite{xing2015normalized} show that results can be improved by enforcing an orthogonal constraint on the linear mapping. \\
Recently, \cite{artetxe2017learning} raise an iterative method that aligns the word embedding spaces gradually. In their work, the vocabulary size for learning can be reduced to 25 word pairs and even only with Arabic numerals. The performance is comparable to the learning with large vocabulary dictionary \cite{cao2016distribution} propose a distribution-based model, which is a modified CBOW model minimizing the distribution dissimilarity between source and target embeddings; here, distribution information refers the mean and variance. 
\cite{zhang2017adversarial} put forward  a method using adversarial training without any parallel data. The discriminator tries to distinguish if the embedding is from the source side, the generator aims to learn the mapping from source embedding space to the target one. These methods are fully unsupervised but the performances are far from the supervised training. \cite{DBLP:journals/corr/abs-1710-04087} simplify the adversarial training structure with different loss functions for the generator and discriminator, regularize the linear mapping to be orthogonal, the results get improved significantly. They also propose to use cross-domain similarity local scaling (CSLS) to handle hubness.

\subsection{Unsupervised MT}

As mentioned previously, the lack of parallel corpora motivates people to use monolingual data to improve the MT system. Some researchers use triangulation techniques (\cite{cohn2007machine}) and semi-supervised approaches (\cite{DBLP:journals/corr/ChengXHHWSL16}) to alleviate this issue. But these methods still require parallel corpora.\\
As to fully unsupervised MT, which only takes advantage of monolingual data, \cite{ravi2011deciphering} first consider it as a deciphering problem, where the source language is considered as ciphertext. They also propose iterative expectation-maximization (EM) method and Bayesian decipherment to train this unsupervised model. To solve the bottleneck of such model; mainly the huge memory required to store the candidates search space, \cite{nuhn2012deciphering} limit search candidates according to the word similarity. \cite{nuhn2014decipherment} limit the search space by using and preselection beam search. For the same reason, \cite{kim2017unsupervised} enforce the sparsity with a simple thresholding for the lexicon, also initializing the training with word classes to efficiently boosts the performance. 
Although initially not based on distributional semantics, recent studies show that the use of word embeddings can bring significant improvement in statistical decipherment (\cite{duong2016learning}).\\
For neural MT, \cite{he2016dual} show a bidirectional, iterative way of effectively using monolingual data. More concretely, they train two agents to translate in opposite directions (e.g. French ${\leftrightarrow}$ English) and make them teach each other through a reinforcement learning process. While promising, this approach still requires a small parallel corpora for a warm start.  
\cite{artetxe2017unsupervised} and \cite{lample2017unsupervised} propose two bi-directional unsupervised MT models which totally rely on monolingual corpora in each language only. Both models need to use cross-lingual word embedding to initialize the MT system and train the sequence-to-sequence system with denoising autoencoder. By introducing back-translation techniques, they turn the unsupervised training into a supervised one. The most important principle for these two work is the shared representations. Sentences from different languages are encoded into a shared spaces.
\section{Outline}
The remainder of this thesis is structured as follows. In Section 1.3 we introduce the notations for this thesis. 
Chapter 2 introduces the development of MT systems from statistical models to  neural models. Some basic techniques and principles for unsupervised MT are also explained.  Chapter 3 gives a survey of training and applications details of cross-lingual word embedding. Chapter 4 discusses the unsupervised learning of cross-lingual word embedding and our efforts on data-driven training. We describe our unsupervised MT system in Chapter 5, which contains mainly context-aware search with the help of language model and denoising autoencoder method aimed at reordering. We demonstrate experimental results of cross-lingual word embedding model and unsupervised machine system in Chapter 6, comparing the performance with the state-of-the-art model. In Chapter 7, we summarize our work and draw conclusions.


\section{Notation}
In this thesis, we use the following notations:
\begin{itemize}
	\item Source sentence:  ${f_1^J:= f_1 \cdots  f_j \cdots f_J}$ 
	\item Target sentence:  ${e_1^I:= e_1 \cdots, e_i \cdots e_I}$
	\item Parallel sentences: $(e_1^I, f_1^J)$
	\item Probabilities of denoising autoencoders: $p_{f\rightarrow f}$, $p_{e\rightarrow e}$
	\item Translation models for both directions: $\Theta_{f\rightarrow e}$, $\Theta_{e \rightarrow f}$
	\item Probabilities of translation models for both directions: $p_{f\rightarrow e}$, $p_{e\rightarrow f}$
	\item Loss for denoising autoencoder: $\mathcal{L}^{\text{auto}}$
	\item Loss for back-translation: $\mathcal{L}^{\text{back}}$
	\item Single character for each language: $f,e$ 
	\item Source and target word embedding: $\bm{f}$, $\bm{e}$
	\item Seed bilingual dictionary: $dico$ 
	\item Corresponding embedding pair: $(\bm{f}, \bm{e})$
	\item Alignment: a
	\item Source and target embedding matrices: $\bm{F}$, $\bm{E}$
	\item Source and target Corpora: $\mathcal{F}, \mathcal{E}$
	\item Mapping default from source to target embedding space: $W$
	\item Mapping with specific direction: $W_{f\rightarrow e}$, $W_{e\rightarrow f}$
	\item Internal states of decoder: $\bm{s}$ and $\bm{s}_i$  
	\item Internal states of encoder: $\bm{h}$ and $\bm{h}_i$ 
	\item Context vectors in attention mechanism: $\bm{c}$ and $\bm{c}_i$ 
	\item Learnable matrices related with context vector, hidden state and alignment: $W_c, W_s, W_a$ 
	\item Cross-lingual regularization term $\Omega(\cdot)$
	\item Loss function for source and target embedding: $\mathcal{L}^f$, $\mathcal{L}^e$
	\item Temperature in inverted softmax: $\beta$ 
	\item General scaling parameter: $\lambda$
	\item Scaling parameters for lexicon and languagel model respectively: $\lambda_{lex}$, $\lambda_{LM}$
	\item Expectation: $\mathbb{E}$
	\item Dimension of embedding: $d$
	\item Neighbors of source and target cross-lingual embeddings: $\mathcal{N}_{e}(\bm{f})$, $\mathcal{N}_{f}(\bm{e})$, 
	\item Frobenius norm: ${\lVert \cdot \rVert}_{\text{F}}$
	\item  Manifold of orthogonal matrices: $ O_d(\mathbb{R})$
	\item Language model for source and target size: $\textbf{LM}_f$, $\textbf{LM}_e$
	\item Similarity matrices for source and target respectively: $M_F$,$M_E$
	\item Vocabulary size for most frequent words ${\lvert \tilde{V} \rvert}$
	\item Permutation distance: $d_{per}$
	\item Probability of deletion noise: $p_{del}$, of insertion noise: $p_{ins}$
	\item Vocabulary size for insertion noise  $V_{ins}$  
	\item Model score not exact the probability, especially in weighted model: $Q(\cdot)$
	\item Feature functions: $q(\cdot)$
	\item Discounting coefficient in phrase detection: $\delta$
\end{itemize}










