
\chapter{Introduction}
%Different from image and audio processing, objective can be the compound of 
%
%Word embedding attached more for natural language processing, for example sentiment analysis and machine translation in recent years.
%Actually maybe 'dog' may have some common activities description as 'cat'. If we can not capture such relationship, we need to run the algorithm on much more data or need human efforts like labeling or grammar analysis since representing word as unique, discrete ids leads to data sparsity. 

 
Neural machine translation (NMT) has recently become the dominant method to machine translation task. In comparison to the traditional statistical machine translation (SMT), NMT systems are trained end-to-end, taking advantages of the continuous representation of the hidden states that greatly alleviate the sparsity problem and make better use of more contextual information. Building a good machine translation system requires a huge collections of parallel data. NMT systems often fail when the training data is not enough.  The lack of parallel corpora is actually a problem. Parallel corpora are costly to build because they requires huge human labor and time and sometime expertise of corresponding languages. Conversely, the monolingual corpora is readily available.\\
\indent Several approaches has been proposed to alleviate this issue, for instance, triangulation and semi-supervised learning techniques, how these systems still need a strong cross-lingual signal. Unsupervised machine translation uses only monolingual data of the source and target language to train the model.\\
\indent Recently the work on cross-lingual word embedding can help to eliminate the need of cross-lingual information. Cross-lingual word embedding can be defined as the word embedding of multiple languages in a joint embedding space. In our work, we consider the learning of cross-lingual word embedding space as the task of learning the mapping from source embedding space to target embedding space.\\
\indent In this work, we propose a simple yet efficient unsupervised machine translation system based on cross-lingual word embedding, we build the system by combining the context-aware beam search and using denoising autoencoder to handle reordering. Our system surpasses the state-of-the-art performance of unsupervised machine translation systems without iterative training. We also analyze the effect of detailed model parameters. We analyze the factors for cross-lingual word embedding training, propose a novel unsupervised training method using stochastic gradient descent instead of popular Procrustes training. We demonstrate that our algorithm can achieve the same performance with the state-of-the-art unsupervised methods. 

For future work we hope to implement an iterative algorithm to polish our unsupervised machine translation model 
\section{Related Work}

\subsection{Word Embedding}
\indent Traditional language processing systems treat words as discrete atomic symbols, they assign for each word a specific id number. Such encoding are arbitrary, and it does not provide any information about the relations that may exist between individual symbols. In comparison, with learning algorithm each word can be represented in high dimensional continuous space. According to the distributional hypothesis, similar words tends to occurs with similar neighbors, so similar words have similar word word representation.  \cite{mikolov2013exploiting} first noticed that continuous embedding spaces exhibit similar structures across languages, even when for distant language pairs for example English-Vietnamese. \\

There are many successful methods \cite{mikolov2013efficient} \cite{mikolov2013distributed}for continuous distributed representation of words. Exploiting word occurrence statistics, word vectors reflect the semantic similarities and dissimilarities. Similar words are close in the embedding space. \cite{pennington2014glove}

 
\cite{bojanowski2016enriching}


\cite{mikolov2013exploiting} first noticed that continuous word embedding space exhibits similar structures according languages, even for distant language pairs. They proposed to learn similarity by learning a linear mapping from a source to a target embedding space. They employed a parallel vocabulary of five thousand words as anchor points to learn this mapping and evaluated their approach on a word translation task.

In practice, Mikolov found that the similarity cross languages can be represented by a linear transformation.\cite{xing2015normalized} showed that results can be improved by enforce an orthogonal constraint on the the linear mapping.

Recently, \cite{artetxe2017learning} proposed an iterative method that align the word embedding spaces gradually. He still need a parallel vocabulary like digits or to start the procedure.

\cite{cao2016distribution}
 \cite{zhang2017adversarial} proposed a method using adversarial training without any parallel data. The discriminator tried to discriminate if the embedding from the source side and the generator aimed to learning the mapping from source embedding space to target one.



\cite{conneau2017word} viewed the word translation task as a word retrieval task and find the nearest neighbor searching suffers from the hubness problem. They further proposed to use cross-domain similarity local scaling (CSLS) penalize the hubs and a validation criterion for unsupervised model selection.


\subsection{Unsupervised Machine Translation}

As mentioned previously, the lack of parallel corpora motivate people use more information for example 
Some researchers tried to use triangulation techniques and semi-supervised approaches to address.  


\cite{cohn2007machine} \cite{chen2017teacher}

There are considerable works on statistical decipherment  to induce a machine translation model from monolingual data.   
\cite{ravi2011deciphering} first proposed some idea about the unsupervised machine problem, he considered the unsupervised translation as a deciphering problem. The source language is considered as ciphertext He also proposed iterative EM method and Bayesian decipherment to learn the unsupervised model. To solve the memory bottleneck of such model, in further work \cite{nuhn2012deciphering} tried to according to context similarity to limit search candidates \cite{nuhn2014decipherment}, beam search and preselection search was combined to limit the search space. \cite{kim2017unsupervised} enforced the sparsity with a simple threshold for the lexicon. He also tried to initialize the lexicon training with word classes, which efficiently boosts the performance. 

Although initially not based on distributional semantics, recent studies show that the use to word embeddings can bring significant improvement in statistical decipherment \cite{duong2016learning}


\cite{he2016dual} made an important contribution to train neural model for unsupervised machine translation. More concretely, their method trains two agents to trainlate in opposite directions (e.g. French ${\rightarrow}$ English and English ${\rightarrow}$ French) and make them teach each other through a reinforcement learning process. While promising, this approach still requires a small parallel corpora for a warm start.  
\cite{artetxe2017unsupervised} \cite{lample2017unsupervised} proposed two bi-directional unsupervised machine translation model which totally rely only on monolingual corpora in each language. These two models both need to use cross-lingual word embedding to initialize the MT system and, train the sequence-to-sequence system as denoising autoencoder and turn the unsupervised training into a supervised one by introducing back-translation techniques.
\cite{lample2018phrase} proposed two model variants, a neural and a phrase-based model. These models have fewer hyper-parameters. And these models performs well also on some distant and low-resource languages.
\section{Outline}
In summary, this work makes the following main contributions:

The remainder of this thesis is structured as follows. In Section 1.3 we introduce 
Chapter 2 introduces the development of machine translation systems from statistical models to to neural models. Some basic techniques and principles for machine translation are also included.  Chapter 3 gives more information,  I do a survey of training and applications details of cross-lingual word embedding. We discuss the unsupervised learning of cross-lingual word embedding and our efforts on data-drive training. We describe our unsupervised machine translation model in chapter 5, which contains mainly context-aware part: with the help of language model and denoising autoencoder aimed at reordering. We demonstrate the results of cross-lingual word embedding model and unsupervised machine model. We will compare the performance with the state-of-the-art model. In Chapter 7 we summarize our work which helps better understanding of learning the corss-lingual word embedding and its usage in translation domain.


\section{Notation}
Normally we denote
\begin{itemize}
	\item source sentence  ${f_1^J:= f_1 \cdots  f_j \cdots f_J}$ 
	\item target sentence  ${e_1^I:= e_1 \cdots, e_i \cdots e_I}$
	\item single character $e,f $ separately
	\item sentences $e_1^N, f_1^N$ when word-by-word translating
	\item $\bm{e}$ and $\bm{f}$ the source and target word embedding
	\item $\bm{E}$, $\bm{F}$ are the corresponding embedding matrices
	\item source and target Corpora $\mathcal{S}, \mathcal{T}$
	\item model probability with subscripts $s$, $t$ such as: \\ $P_{s\rightarrow s}(\cdots)$, $P_{t \rightarrow t }(\cdots)$ for denoising autoencoder, $P_{s\rightarrow t}(\cdots)$, $P_{t\rightarrow s}(\cdots)$ for translation model

\end{itemize}










