
\chapter{Introduction}
%Different from image and audio processing, objective can be the compound of 
%
%Word embedding attached more for natural language processing, for example sentiment analysis and machine translation in recent years.
%Actually maybe 'dog' may have some common activities description as 'cat'. If we can not capture such relationship, we need to run the algorithm on much more data or need human efforts like labeling or grammar analysis since representing word as unique, discrete ids leads to data sparsity. 

 
Neural machine translation (NMT) has recently become the dominant method to machine translation task. In comparison to the traditional statistical machine translation (SMT), NMT systems are trained end-to-end, taking advantages of the continuous representation of the hidden states that greatly alleviate the sparsity problem and make better use of more contextual information. Building a good machine translation system requires a huge collections of parallel data. NMT systems often fail when the training data is not enough.  The lack of parallel corpora is actually a problem. Parallel corpora are costly to build because they requires huge human labor and time and sometime expertise of corresponding languages. Conversely, the monolingual corpora is readily available.\\
\indent Several approaches has been proposed to alleviate this issue, for instance, triangulation and semi-supervised learning techniques, how these systems still need a strong cross-lingual signal. Unsupervised machine translation uses only monolingual data of the source and target language to train the model.\\
\indent Recently the work on cross-lingual word embedding can help to eliminate the need of cross-lingual information. Cross-lingual word embedding can be defined as the word embedding of multiple languages in a joint embedding space. In our work, we consider the learning of cross-lingual word embedding space as the task of learning the mapping from source embedding space to target embedding space.\\
\indent In this work, we propose a simple yet efficient unsupervised machine translation system based on cross-lingual word embedding, we build the system by combining the context-aware beam search and using denoising autoencoder to handle reordering. Our system surpasses the state-of-the-art performance of unsupervised machine translation systems without iterative training. We also analyze the effect of detailed model parameters. We analyze the factors for cross-lingual word embedding training, propose a novel unsupervised training method using stochastic gradient descent instead of popular Procrustes training. We demonstrate that our algorithm can achieve the same performance with the state-of-the-art unsupervised methods. 

For future work we hope to implement an iterative algorithm to polish our unsupervised machine translation model 
\section{Related Work}

\subsection{Word Embedding}
\indent Traditional language processing systems treat words as discrete atomic symbols, they assign for each word a specific id number. Such encodings are arbitrary, and it does not provide any information about the relations that may exist between individual symbols. In comparison, with learning algorithm each word can be represented in high dimensional continuous space. According to the distributional hypothesis, similar words tends to occurs with similar neighbors, so similar words have similar word representations.  \cite{mikolov2013exploiting} first noticed that continuous embedding spaces exhibit similar structures across languages, even when for distant language pairs for example English-Vietnamese. \\
The series of researches about word embedding started from \cite{mikolov2013efficient}. In this work, he just proposed two famous structures: continuous bag-of-words model (CBOW) and the skip-gram model to learn the word embedding and such embeddings have good properties for measure the syntactic and semantic word similarities and in his further work: \cite{mikolov2013distributed}, he discussed about learning the embedding for phrases. \cite{pennington2014glove} proposed a regression model, which combines the global matrix factorization and local context window methods, make the training process more interpretable. \cite{bojanowski2016enriching} tried to represent each word as a bag of character $n$-grams, he assigned for each character $n$-gram a distinct embedding, in this way, we can exploit the subword information. \\
\cite{mikolov2013exploiting} first noticed that continuous word embedding space exhibits similar structures according languages, even for distant language pairs, which is also the start work for cross-lingual word embedding, and this works also inspired the work for unsupervised machine translation, since they can provide heuristic information. In detail they tried to learn similarity by learning a linear mapping from a source to a target embedding space. They employed a parallel vocabulary of five thousand words as anchor points to learn this mapping and evaluated their approach on a word translation task. \cite{xing2015normalized} showed that results can be improved by enforce an orthogonal constraint on the linear mapping.\\
Recently, \cite{artetxe2017learning} proposed an iterative method that align the word embedding spaces gradually. Thanks to his contribution, the vocabulary size for learning embeddings can be reduced to 25 word pairs and even with Arabic numerals. The performance is comparable to the learning with large vocabulary dictionary

\cite{cao2016distribution} proposed a distribution-based model, which is a modified CBOW model which tries to minimization between the distribution dissimilarity between source and target embeddings. Here the distribution refers the mean and variance. 
\cite{zhang2017adversarial} proposed a method using adversarial training without any parallel data. The discriminator tried to discriminate if the embedding from the source side and the generator aimed to learning the mapping from source embedding space to target one. These methods are totally unsupervised but the performances are far worse than the supervised training. \cite{conneau2017word} simplifies the adversarial training structure with different loss function for generator and discriminator. The performance got much improved, in addition, he viewed the word translation task as a word retrieval task and find the nearest neighbor searching suffers from the hubness problem. They further proposed to use cross-domain similarity local scaling (CSLS) penalize the hubs and a validation criterion for unsupervised model selection.


\subsection{Unsupervised Machine Translation}

As mentioned previously, the lack of parallel corpora motivate people to use monolingual data to improve the machine translation system. Some researchers tried to use triangulation techniques \cite{cohn2007machine} and semi-supervised approaches \cite{cheng2016semi}to address. But these methods still require parallel corpora.\\
As to total unsupervised machine translation, which only takes advantages of monolingual data,\cite{ravi2011deciphering} first proposed some ideas about the unsupervised machine model, he considered the unsupervised translation as a deciphering problem, where the source language is considered as ciphertext. They also proposed iterative EM method and Bayesian decipherment to train this unsupervised model. To solve the bottleneck of such model, mainly the huge memory is required to store the candidates search space, Based on deciphering model, \cite{nuhn2012deciphering} tried to limit search candidates according to the word similarity. \cite{nuhn2014decipherment} limited the search space by using beam search and preselection search. \cite{kim2017unsupervised} enforced the sparsity with a simple threshold for the lexicon. He also tried to initialize the lexicon training with word classes, which efficiently boosts the performance. 
Although initially not based on distributional semantics, recent studies show that the use of word embeddings can bring significant improvement in statistical decipherment \cite{duong2016learning}.\\
\cite{he2016dual} made an important contribution to train neural model for unsupervised machine translation. More concretely, their method trains two agents to translate in opposite directions (e.g. French ${\rightarrow}$ English and English ${\rightarrow}$ French) and make them teach each other through a reinforcement learning process. While promising, this approach still requires a small parallel corpora for a warm start.  
\cite{artetxe2017unsupervised} \cite{lample2017unsupervised} proposed two bi-directional unsupervised machine translation model which totally rely only on monolingual corpora in each language. These two models both need to use cross-lingual word embedding to initialize the MT system and, train the sequence-to-sequence system as denoising autoencoder and turn the unsupervised training into a supervised one by introducing back-translation techniques. The most important principle for these two work are the sharing embedding space. Sentences from different languages are encoded into a shared embedding spaces and then translated into specific language with different decoder.
\cite{lample2018phrase} proposed two model variants, a neural and a phrase-based model. These models have fewer hyper-parameters. And these models performs well also on some distant and low-resource languages.
\section{Outline}
In summary, this work makes the following main contributions:

The remainder of this thesis is structured as follows. In Section 1.3 we introduce 
Chapter 2 introduces the development of machine translation systems from statistical models to to neural models. Some basic techniques and principles for machine translation are also included.  Chapter 3 gives more information,  I do a survey of training and applications details of cross-lingual word embedding. We discuss the unsupervised learning of cross-lingual word embedding and our efforts on data-drive training. We describe our unsupervised machine translation model in chapter 5, which contains mainly context-aware part: with the help of language model and denoising autoencoder aimed at reordering. We demonstrate the results of cross-lingual word embedding model and unsupervised machine model. We will compare the performance with the state-of-the-art model. In Chapter 7 we summarize our work which helps better understanding of learning the corss-lingual word embedding and its usage in translation domain.


\section{Notation}
Normally we denote
\begin{itemize}
	\item source sentence  ${f_1^J:= f_1 \cdots  f_j \cdots f_J}$ 
	\item target sentence  ${e_1^I:= e_1 \cdots, e_i \cdots e_I}$
	\item single character $e,f $ separately
	\item sentences $e_1^N, f_1^N$ when word-by-word translating
	\item $\bm{e}$ and $\bm{f}$ the source and target word embedding
	\item $\bm{E}$, $\bm{F}$ are the corresponding embedding matrices
	\item source and target Corpora $\mathcal{S}, \mathcal{T}$
	\item model probability with subscripts $s$, $t$ such as: \\ $P_{s\rightarrow s}(\cdots)$, $P_{t \rightarrow t }(\cdots)$ for denoising autoencoder, $P_{s\rightarrow t}(\cdots)$, $P_{t\rightarrow s}(\cdots)$ for translation model

\end{itemize}










