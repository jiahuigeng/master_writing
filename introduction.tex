
\chapter{Introduction}
Building a good machine translation system requires a huge collections of parallel data. NMT systems often fail when the training data is not enough.  The lack of parallel corpora is actually a problem. Parallel corpora are costly because they require huge human labor, time and expertise of corresponding languages.
\indent Several approaches has been proposed to alleviate this issue, for instance, triangulation and semi-supervised learning techniques, however these systems still need a strong cross-lingual signal. Unsupervised machine translation uses only monolingual data of the source and target language to train the model and such monolingual corpora is readily available.\\
Recently cross-lingual word embedding draws more and more attention since it helps to exploit the multilingual semantic information, furthermore enables computation of cross-lingual word similarities, which is relevant for many tasks such as bilingual dictionary induction or cross-lingual information retrieval. Several methods are proposed to learn the cross-lingual without any parallel data. This inspires the works on unsupervised machine translation as We can make use of such cross-lingual information in an unsupervised manner.\\
In this thesis, I first make a survey of the current training algorithm of cross-lingual word embedding, find the significant training skills among them and their respective advantages and disadvantages. Then I propose a novel data-based unsupervised training method, the experiments the method can achieve considerable results  in comparison with the start-of-the-art method. I further explore the 
application of the cross-lingual word embeddings in the machine translation domain.
I develop a total unsupervised machine translation system starting from the simple word-by-word translation principle, to over the shortcomings to such more, I design the context-aware beam search to find the best translation candidate. To handle to reordering, I implement a denoising network with artificial noises, which are to mimic the true noise in the word-by-word translation, The results demonstrate that such simple but efficient translation system can surpass the most unsupervised neural machine translation system with costly iterative training.


\section{Related Work}

\subsection{Word Embedding}
\indent Traditional language processing systems treat words as discrete atomic symbols: they assign for each word a specific id number. Such encodings are arbitrary, and it does not provide any information about the relations that may exist between individual symbols. They are at their limits in many tasks. In comparison, the distributed representations of words in high dimensional continuous space help learning algorithm to achieve better performance in natural language processing. According to the distributional hypothesis, similar words tends to occurs with similar neighbors, so similar words have similar word representations. At the beginning, in order to explore word similarity, those training algorithms involve dense matrix multiplication and complex matrix analysis. \\
\cite{mikolov2013efficient} propose two models, namely skip-gram model and continuous bag of words model (CBOW). They used neural architectures for learning word vectors, which makes training extremely efficient. In further work, \cite{mikolov2013distributed} discuss about learning the embedding for phrases. \cite{pennington2014glove} propose a regression model, which combines the global matrix factorization and local context window methods, make the training process more interpretable. \cite{bojanowski2016enriching} represent each word as a bag of character $n$-grams, and assign for each character $n$-gram a distinct embedding. In this way, we can exploit the subword information. \\
\cite{mikolov2013exploiting} again notice that continuous embedding spaces exhibit similar structures across languages, even when for distant language pairs, for example, English-Vietnamese.  With separately learned word embeddings he is committed to exploiting semantic word similarities, concretely by learning a linear mapping from source embedding space to target embedding space. They employ a parallel vocabulary of five thousand words as anchor points to learn this mapping and evaluate their approach on a word translation task. \cite{xing2015normalized} show that results can be improved by enforce an orthogonal constraint on the linear mapping. \\
Recently, \cite{artetxe2017learning} raise an iterative method that align the word embedding spaces gradually. Thanks to his contribution, the vocabulary size for learning can be reduced to 25 word pairs and even with Arabic numerals. The performance is comparable to the learning with large vocabulary dictionary \cite{cao2016distribution} propose a distribution-based model, which is a modified CBOW model which tries to minimization between the distribution dissimilarity between source and target embeddings. Here distribution information refers the mean and variance. 
\cite{zhang2017adversarial} put forward  a method using adversarial training without any parallel data. The discriminator tried to discriminate if the embedding from the source side and the generator aimed to learning the mapping from source embedding space to target one. These methods are totally unsupervised but the performances are far worse than the supervised training. \cite{conneau2017word} simplifies the adversarial training structure with different loss functions for generator and discriminator. The performance got much improved, They also proposed to use cross-domain similarity local scaling (CSLS) to handle hubness and a validation criterion for unsupervised model selection.


\subsection{Unsupervised Machine Translation}

As mentioned previously, the lack of parallel corpora motivate people to use monolingual data to improve the machine translation system. Some researchers tried to use triangulation techniques (\cite{cohn2007machine}) and semi-supervised approaches (\cite{cheng2016semi}) to alleviate this issue. But these methods still require parallel corpora.\\
As to fully unsupervised machine translation, which only takes advantages of monolingual data, \cite{ravi2011deciphering} first consider it as a deciphering problem, where the source language is considered as ciphertext. They also proposed iterative expectation-maximization method  (EM) and Bayesian decipherment to train this unsupervised model. To solve the bottleneck of such model mainly the huge memory required to store the candidates search space,  \cite{nuhn2012deciphering} limit search candidates according to the word similarity. \cite{nuhn2014decipherment} limit the search space by using beam search and preselection search. \cite{kim2017unsupervised} enforce the sparsity with a simple threshold for the lexicon. He also initialize the lexicon training with word classes, which efficiently boosts the performance. 
Although initially not based on distributional semantics, recent studies show that the use of word embeddings can bring significant improvement in statistical decipherment \cite{duong2016learning}.\\
\cite{he2016dual} raise dual learning for neural machine translation. More concretely, they train two agents to translate in opposite directions (e.g. French ${\rightarrow}$ English and English ${\rightarrow}$ French) and make them teach each other through a reinforcement learning process. While promising, this approach still requires a small parallel corpora for a warm start.  
\cite{artetxe2017unsupervised} and \cite{lample2017unsupervised} propose two bi-directional unsupervised machine translation model which totally rely only on monolingual corpora in each language. These two models both need to use cross-lingual word embedding to initialize the MT system and train the sequence-to-sequence system with denoising autoencoder. They turn the unsupervised training into a supervised one by introducing back-translation techniques. The most important principle for these two works are the shared embedding space. Sentences from different languages are encoded into a shared embedding spaces and then translated into specific language with different decoder.
\section{Outline}
The remainder of this thesis is structured as follows. In Section 1.3 we introduce the notations in this thesis. 
Chapter 2 introduces the development of machine translation systems from statistical models to  neural models. Some basic techniques and principles for machine translation are also included.  Chapter 3 gives more information,  I do a survey of training and applications details of cross-lingual word embedding. We discuss the unsupervised learning of cross-lingual word embedding and our efforts on data-driven training. We describe our unsupervised machine translation model in chapter 5, which contains mainly context-aware method with the help of language model and denoising autoencoder method aimed at reordering. We demonstrate the results of cross-lingual word embedding model and unsupervised machine model. We will compare the performance with the state-of-the-art model. In Chapter 7 we summarize our work.


\section{Notation}
In this thesis, we use the following notations:
\begin{itemize}
	\item source sentence  ${f_1^J:= f_1 \cdots  f_j \cdots f_J}$ 
	\item target sentence  ${e_1^I:= e_1 \cdots, e_i \cdots e_I}$
	\item single character $e,f $
	\item $\bm{f}$ and $\bm{e}$ the source and target word embedding
	\item $\bm{E}$, $\bm{F}$ are the corresponding embedding matrices
	\item source and target Corpora $\mathcal{F}, \mathcal{E}$
	\item $P(e_1^I | e_1^J)$ and ${P(f_1^J |f_1^I)}$ for denoising autoencoder
	\item $P(e_1^I | f_1^J)$ and ${P(f_1^J |e_1^I)}$ for translation model
	\item $\bm{s}$ and $\bm{s}_i$ for hidden states of RNN encoder
	\item $\bm{h}$ and $\bm{h}_i$ for hidden states of RNN decoder
	\item $\bm{c}$ and $\bm{c}_i$ for context vectors
	\item $\bm{\alpha}_i(j)$: the alignment between the $i$-th decoder sate and $j$-th encoder state
	\item $\tilde{\bm{h}}_i$:  attentional hidden state
	hidden state
\end{itemize}










