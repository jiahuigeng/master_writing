
\chapter{Introduction}
Building a good machine translation system requires a enormous collection of parallel data. NMT systems often fail when the training data is not enough.  However parallel corpora are costly because they require huge human labor, time and expertise of corresponding languages.
\indent Several approaches has been proposed to alleviate this issue, for instance, triangulation or semi-supervised learning techniques; these systems however still need a strong cross-lingual signal. Unsupervised machine translation uses only monolingual data of the source and target languages to train the model and such monolingual corpora are readily available.\\
Recently cross-lingual word embedding draws more and more attention since it helps to exploit the multilingual semantic information. For instance, it enables computation of cross-lingual word similarities, which  can be directly applied to bilingual dictionary induction or cross-lingual information retrieval. Several methods are proposed to learn the cross-lingual without any parallel data. This correspondingly inspires work on unsupervised machine translation.\\
In this thesis, I first make a survey of the current training algorithm of cross-lingual word embedding, find the significant training skills among them and their respective advantages and disadvantages. Then I propose a novel data-based unsupervised training method, the experiments the method can achieve considerable results  in comparison with the start-of-the-art method. I further explore the 
application of the cross-lingual word embeddings in the machine translation domain.
I develop a total unsupervised machine translation system starting from the simple word-by-word translation principle, to over the shortcomings to such more, I design the context-aware beam search to find the best translation candidate. To handle to reordering, I implement a denoising network with artificial noises, which are to mimic the true noise in the word-by-word translation, The results demonstrate that such simple but efficient translation system can surpass the most unsupervised neural machine translation system with costly iterative training.


\section{Related Work}

\subsection{Word Embedding}
\indent Traditional language processing systems treat words as discrete atomic symbols, which assign for each word a specific ID number. Such encodings are arbitrary; it does not provide any information about the relations that may exist between individual symbols. In comparison, distributed representations of words in a high dimensional continuous space help learning algorithm to achieve better performance in natural language processing. According to the distributional hypothesis, similar words tend to occurs with semantic neighbors, having similar word representations. At the beginning, in order to explore word similarity, word embedding training involves dense matrix multiplication and complex matrix analysis. \\
\cite{mikolov2013efficient} propose two models for learning word embeddings with neural networks, namely skip-gram and continuous bag of words (CBOW) models. Neural architectures makes the training extremely efficient. In further work, \cite{mikolov2013distributed} discuss about learning the embedding for phrases. \cite{pennington2014glove} propose a regression model which combines the global matrix factorization and local context window methods, making the training process more interpretable. \cite{bojanowski2016enriching} represent each word as a bag of character $n$-grams, and assign for each character $n$-gram a distinct embedding. In this way, they can exploit subword information in word embedding training. \\
\cite{mikolov2013exploiting} again notice that continuous embedding spaces exhibit similar structures across languages, even when for distant language pairs, for example, English-Vietnamese.  With separately learned word embeddings they learn a linear mapping from source embedding space to target embedding space. To do this, they employ a parallel vocabulary of five thousand words as anchor points and evaluate their approach on a word translation task. \cite{xing2015normalized} show that results can be improved by enforcing an orthogonal constraint on the linear mapping. \\
Recently, \cite{artetxe2017learning} raise an iterative method that aligns the word embedding spaces gradually. In their work, the vocabulary size for learning can be reduced to 25 word pairs and even with Arabic numerals. The performance is comparable to the learning with large vocabulary dictionary \cite{cao2016distribution} propose a distribution-based model, which is a modified CBOW model minimizing the distribution dissimilarity between source and target embeddings; here, distribution information refers the mean and variance. 
\cite{zhang2017adversarial} put forward  a method using adversarial training without any parallel data. The discriminator tries to distinguish if the embedding is from the source side, the generator aims to learn the mapping from source embedding space to the target one. These methods are totally unsupervised but the performances are far worse than the supervised training. \cite{conneau2017word} simplifies the adversarial training structure with different loss functions for the generator and discriminator, improving the performance significantly, They also propose to use cross-domain similarity local scaling (CSLS) to handle hubness and a validation criterion for unsupervised model selection.


\subsection{Unsupervised Machine Translation}

As mentioned previously, the lack of parallel corpora motivates people to use monolingual data to improve the machine translation system. Some researchers use triangulation techniques (\cite{cohn2007machine}) and semi-supervised approaches (\cite{cheng2016semi}) to alleviate this issue. But these methods still require parallel corpora.\\
As to fully unsupervised machine translation, which only takes advantage of monolingual data, \cite{ravi2011deciphering} first consider it as a deciphering problem, where the source language is considered as ciphertext. They also propose iterative expectation-maximization (EM) method and Bayesian decipherment to train this unsupervised model. To solve the bottleneck of such model -- mainly the huge memory required to store the candidates search space, -- \cite{nuhn2012deciphering} limit search candidates according to the word similarity. \cite{nuhn2014decipherment} limit the search space by using and preselection beam search. For the same reason, \cite{kim2017unsupervised} enforce the sparsity with a simple thresholding for the lexicon, also initializing the training with word classes to efficiently boosts the performance. 
Although initially not based on distributional semantics, recent studies show that the use of word embeddings can bring significant improvement in statistical decipherment \cite{duong2016learning}.\\
For neural machine translation, \cite{he2016dual} show a bidirectional, iterative way of effectively using monolingual data. More concretely, they train two agents to translate in opposite directions (e.g. French ${\rightarrow}$ English and English ${\rightarrow}$ French) and make them teach each other through a reinforcement learning process. While promising, this approach still requires a small parallel corpora for a warm start.  
\cite{artetxe2017unsupervised} and \cite{lample2017unsupervised} propose two bi-directional unsupervised machine translation model which totally rely only on monolingual corpora in each language. Both models  need to use cross-lingual word embedding to initialize the MT system and train the sequence-to-sequence system with denoising autoencoder. They turn the unsupervised training into a supervised one by introducing back-translation techniques. The most important principle for these two work is the shared representations. Sentences from different languages are encoded into a shared spaces and then translated into specific languages with different decoders.
\section{Outline}
The remainder of this thesis is structured as follows. In Section 1.3 we introduce the notations for this thesis. 
Chapter 2 introduces the development of machine translation systems from statistical models to  neural models. Some basic techniques and principles for unsupervised machine translation are also explained.  Chapter 3 gives a survey of training and applications details of cross-lingual word embedding. Chapter 4 discusses the unsupervised learning of cross-lingual word embedding and our efforts on data-driven training. We describe our unsupervised machine translation system in Chapter 5, which contains mainly context-aware search with the help of language model and denoising autoencoder method aimed at reordering. We demonstrate experimental results of cross-lingual word embedding model and unsupervised machine system in Chapter 6, comparing the performance with the state-of-the-art model. In Chapter 7, we summarize our work and draw conclusions.


\section{Notation}
In this thesis, we use the following notations:
\begin{itemize}
	\item Source sentence:  ${f_1^J:= f_1 \cdots  f_j \cdots f_J}$ 
	\item Target sentence:  ${e_1^I:= e_1 \cdots, e_i \cdots e_I}$
	\item Single character for each language: $f,e$ 
	\item Source and target word embedding: $\bm{f}$, $\bm{e}$
	\item Source and target embedding matrices: $\bm{F}$, $\bm{E}$
	\item Source and target Corpora: $\mathcal{F}, \mathcal{E}$
	\item Mapping default from source to target embedding space: $W$
	\item Mapping with specific direction: $W_{f\rightarrow e}$, $W_{e\rightarrow f}$
	\item Internal states of decoder: $\bm{s}$ and $\bm{s}_i$  
	\item Internal states of encoder: $\bm{h}$ and $\bm{h}_i$ 
	\item Context vectors in attention mechanism: $\bm{c}$ and $\bm{c}_i$ 

\end{itemize}










