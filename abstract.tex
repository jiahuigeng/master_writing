\chapter{Abstract}
In recent years, cross-lingual representation of words enable us to explore the structures of different languages in a shared space. Based on word similarity in a cross-lingual space. we can realize bilingual dictionary induction, information retrieval and knowledge transfer between resource-rich and resource-lean languages.\\
%In this thesis we first make a survey of the cross-lingual word embedding training methods and its application in unsupervised machine translation. Then we proposed a novel training method of cross-lingual word embedding, which is data-driven and supported with language model, we can better utilize the context information for training. we further exploit the cross-lingual similarity in word-by-word based unsupervised machine translation combined with context-aware beam search and denoising autoencoder. \\
%The performance of the cross-lingual embedding is measuared on an open dictionary from Facebook, we find the performance is comparable with the state-of-the-art supervised and unsupervised methods. Based on that, our simple yet efficient unsupervised machine translation system can produce meaningful machine translation the BLEU scores even get beyond some unsupervised system with costly iterative training. \\
%We also make some ablation studies, analyze the effects of various factors in the cross-lingual word embedding training and various artificial noise. We think our work can provide better understanding of the training and the applications of word embedding in machine translation (MT).
In this thesis, I propose a novel training method of cross-lingual word embedding, which is distinct from other methods by exploiting an extra language model and its corpus-based instead of vocabulary-based training process.
We can make better use of contextual information. We further combine the translation system with denoising neural network to handle reordering and build a simple yet efficient unsupervised machine translation system. The performance of cross-lingual word embedding is measured on a word retrieval task released by\cite{conneau2017word}. The results are comparable with the current supervised and unsupervised methods. And the simple yet efficient translation system surpasses state-of-the-art unsupervised translation system without costly iteratively training.\\
We verify the cross-lingual embedding on subword units (byte pair encoding, BPE) performs poorly in translation and show that vocabulary cut-off helps for sentence translation. We also analyze the effect of different artificial noises to denoising model and propose a novel noise type.


