\chapter{Abstract}
In recent years, cross-lingual representation of words enable us to explore the structures of different languages in a shared space. Based on word similarity in a cross-lingual space. we can realize bilingual dictionary induction, information retrieval and knowledge transfer between resource-rich and resource-lean languages.\\
%In this thesis we first make a survey of the cross-lingual word embedding training methods and its application in unsupervised machine translation. Then we proposed a novel training method of cross-lingual word embedding, which is data-driven and supported with language model, we can better utilize the context information for training. we further exploit the cross-lingual similarity in word-by-word based unsupervised machine translation combined with context-aware beam search and denoising autoencoder. \\
%The performance of the cross-lingual embedding is measuared on an open dictionary from Facebook, we find the performance is comparable with the state-of-the-art supervised and unsupervised methods. Based on that, our simple yet efficient unsupervised machine translation system can produce meaningful machine translation the BLEU scores even get beyond some unsupervised system with costly iterative training. \\
%We also make some ablation studies, analyze the effects of various factors in the cross-lingual word embedding training and various artificial noise. We think our work can provide better understanding of the training and the applications of word embedding in machine translation (MT).
In this thesis, I propose a novel training method of cross-lingual word embedding which is data-driven and supported by the language model. We better utilize the context information to find the 
We compare the results with that using BPE as the translation unit in our unsupervised model, find  BPE does not improve the performance.   



