\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\babel@toc {british}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces Illustration of a translation process using IBM-3 model (\cite {koehn2009statistical})}}{9}{figure.2.1}
\contentsline {figure}{\numberline {\relax 2.2}{\ignorespaces English$\rightarrow $ French translation\IeC {\textendash } example of a deep NMT (\cite {luong2015effective}).}}{12}{figure.2.2}
\contentsline {figure}{\numberline {\relax 2.3}{\ignorespaces Global attention (\cite {luong2015effective})}}{14}{figure.2.3}
\contentsline {figure}{\numberline {\relax 2.4}{\ignorespaces The Transformer model architecture (\cite {vaswani2017attention})}}{15}{figure.2.4}
\contentsline {figure}{\numberline {\relax 2.5}{\ignorespaces Self-attention structure}}{16}{figure.2.5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces Global attention model (\cite {mikolov2013efficient})}}{20}{figure.3.1}
\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces A cross-lingual embedding space between German and English (\cite {ruder2017survey})}}{21}{figure.3.2}
\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces Cross-lingual projection with CCA (\cite {faruqui2014improving})}}{23}{figure.3.3}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 5.1}{\ignorespaces Reordering noise}}{37}{figure.5.1}
\contentsline {figure}{\numberline {\relax 5.2}{\ignorespaces Insertion noise}}{38}{figure.5.2}
\contentsline {figure}{\numberline {\relax 5.3}{\ignorespaces Deletion noise}}{39}{figure.5.3}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
