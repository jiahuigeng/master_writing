\chapter{Unsupervised Learning of Cross-lingual Word Embedding}
Chapter 3 introduce the supervised learning of cross-lingual word embedding, they employ parallel data like bilingual dictionary or parallel corpora and  have shown strong results on a word retrieval task.  However, labeling the lexicon information still cost lots of labor. This motivates people to explore fully unsupervised method. In this Chapter, several unsupervised approaches will be explained including a novel data-drive approach, which is distinct from other methods by exploiting an extra language model and corpus-based instead of vocabulary-based training process. These approaches contains always a iterative training procedure. Mapping will be refined until reach an optimum value. 
\section{Initialization}
Pre-trained monolingual embeddings capture distribution statistics for each language. These embeddings are typically not aligned between languages and ince the alignment method employed is iterative, a good initialization is a key.
A practical approach for reducing the need of bilingual supervision is to design heuristics to build the seed dictionary. \cite{} propose to use shared words and cognates to extract the seed dictionary.This method has a deficit that it depends on the writing system.  A recent line of using adversarial training to learn a mapping, which roughly align the two distributions of embeddings is proposed.\\

\subsection{Heuristics}
The seed is constructed by identifying words with similar spelling (cognates). A relative-frequency constraint is imposed to filter out word pairs that have fully different meaning.  This noisy seed lexicon is considered as initial signal. Generate the list of words sorted by frequency first. For each of the most frequent source words, start from the top of the target word frequency list, if the normalized edit distance (NED) and absolute difference between the respective frequency ranks is within a specific threshold value, then add word pair into the seed lexicon. There is no one-to-one constraint, so both source and target words may appear multiple times in the seed. 

\subsection{Adversarial Training (GANs)}
The adversarial training was for cross-lingual embedding mapping is first proposed by {}, who combines an encoder that maps source word embeddings into the target embedding space, a decoder that reconstructs the source embeddings from the mapped embeddings and a discriminator that discruminates between the mapped embeddings and the true target embeddings. \cite{zhang2017adversarial} incorporate additional techniques like noise injections to aid the training. \cite{conneau2017word} drop the reconstruction component, regularize the mapping to be orthogonal.\\
Let ${=\{ \bm{f}_1, \cdots, \bm{f}_\}}$ and ${ = \{ \bm{e}_i, \cdots , \bm{e}_m\}}$ be the two sets of $n$ and $m$ word embeddings from a source and a target language separately, We refer the discriminator parameters as ${\theta_D}$.The discriminator is a multi-layer neural network trained to discriminate the transformed source word embedding from the target word embedding, while the mapping $W$, simply a linear transformation, is trained to fooling discriminator. In the two-player game, we are supposed to learn the mapping from source embedding space to the target space.

Generator loss 
\[ \mathcal{L_G}(W|\theta_D) =  -\frac{1}{n} \sum_{i=1}^{n}\log P_{\theta_D}(source=0|W \bm{f}_i) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_D}(source = 1 | \bm{e}_i) \]
Discriminator loss
\[ \mathcal{L_D}(\theta_D | W) =  -\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta_D}(source = 1| W\bm{f}_i) - \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta_D}(source=0| \bm{e}_i) \]	

%
%These adversarial training methods are reported to be limited to closely-related languages or comparable Wikipedia corpora.
\subsection{Similarity Matrix}


\section{Vocabulary-based Training}
\subsection{Iterative Procrustes Analysis}
After initialization, the words in the seed dictionary can be considered as temporary  anchor points for Procrustes analysis, which has been introduced in Section .
Apply the Procrustes solution on the current seed dictionary, use the learned mapping matrix $W$ and the nearest neighbor search to construct a better seed dictionary again. Repeat this process iteratively to obtain a hopefully better mapping and dictionary each time until some convergence  criterion is met. As for the convergence criterion, the training will be stopped when the improvement on the average dot product for the induced dictionary falls below a give threshold from one iteration to the next. In practice, the threshold value is set at $1e-6$.
\begin{figure}[h]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H]
			\SetAlgoLined
			\KwIn{$F$ (source embeddings)}
			\KwIn{$E$ (target embeddings)}
			\KwIn{$dico$ (seed dictionary)}
			\KwResult{$W$ (embedding mapping) }
			\While{not converge}{
				${W \leftarrow LEARN\_MAPPING(F,E,dico)}$
				${dico \leftarrow LEARN\_DICTIONARY(W)}$
				
			}
			\caption{Iterative training procedure}
		\end{algorithm}
	\end{minipage}
\end{figure}
\subsection{Seed Dictionary Induction}
Specifically, for each word in the most frequent words, we use nearest neighbors to find best candidates. To prevent the mapping and dictionary from getting stuck in local optima, Mutual nearest neighbors and thresholding are used to ensure a high-quality dictionary. In the experiments we found that the number of word pairs in the dictionary is not so important as the dictionary quality. The bidirectional dictionary, which belongs to the intersection of two unidirectional dictionaries, work better than other set operation. 
\section{Corpus-based Training}
\subsection{Motivation}
The vocabulary-based training approach have maken good use to the similarity of embedding distributions. It dose not consider the semantic or contextual information when training the cross-lingual mapping. We propose to make use to corpus translation to learn the mapping, since we do not have parallel sentences and alignment information, we can build a word-based machine translation, which exploits the language model to improve the translation quality. The intuition is that if the sentence translation system, which contains lexicon model, get improved compared with the previous one, we can further learn a better mapping. Repeat this process the lexicon translation and embedding mapping will be trained alternatively, until some convergence criterion is met.  
\begin{figure}[H]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H]
			\SetAlgoLined
			\KwIn{$F$ (source embeddings)}
			\KwIn{$E$ (target embeddings)}
			\KwIn{$\textbf{LM}_e$ (language model)}
			\KwIn{$\mathcal{F}$ (source corpus)}
			\KwResult{$W (embedding mapping)$}
			\While{not converge}{
				${\ \  \mathcal{E} \leftarrow TRANSLATE(\mathcal{F}, F, E, \textbf{LM}_e)}$ 
				${W \leftarrow LEARN\_MAPPING(\mathcal{F}, \mathcal{E})}$				
			}		
			\caption{Iterative learning of corpus-based approach}
		\end{algorithm}
	\end{minipage}
\end{figure}

%We propose to use the output word-by-word translation as the input of the mapping learning system. Assuming that the quality of word-by-word translation was indeed better than the previous one. The mapping learning system should be able to learn a better mapping and, consequently, an ever better translation the second time. The process can then be repeated iteratively to obtain a hopefully better maping and translation each time until some convergence point was met. 
Algorithm 3 summarize this iterative framework I propose.\\
The approach combines embedding mapping learning, dictionary induction technique and word-based machine translation, in this thesis, the word-based translation is simplified to word-by-word translation. However, the efficiency turns out to be critical for this approach. By enclosing the learning logic in a loop, the training time depends mainly on the corpus size. It will be very costly if we translate the entire source corpus. Especially at the beginning stage, the improvement of the mapping is a relative slow process and need more training iteration. It takes too long to translation the whole corpus. I propose an online training method, which can greatly improve the training efficiency. 

\subsection{Online Training}

\begin{figure}[H]
	\centering
	\begin{minipage}{.7\linewidth}
		\begin{algorithm}[H]
			\SetAlgoLined
			\KwIn{$F$ (source embeddings)}
			\KwIn{$E$ (target embeddings)}
			\KwIn{$\textbf{LM}_e$ (language model)}
			\KwIn{$\mathcal{F}$ (source corpus)}
			\KwResult{$W (embedding mapping) $}
			\While{not converge}{
				\ \ Generate batch of source sentences $\{f_1^J\}$ from $\mathcal{F}$\\
				${\{e_1^I\} \leftarrow TRANSLATE(\{e_1^I\}, F, E, \textbf{LM}_e) }$ 
				${W \leftarrow LEARN\_MAPPING(\{f_1^J\}, \{e_1^I\})}$				
			}		
			\caption{Online learning for corpus-based approach}
		\end{algorithm}
	\end{minipage}
\end{figure}
Instead of translate the entire corpus, I iterate over the corpus to generate specific number of sentences as a batch for translation. After translating, we extract the word translation pairs as seed dictionary. For more complicated word-based machine translation system, extraction will also requires the alignment information. In word-by-word translation system, we just use the translation pairs at each position. We could improve the quality of the seed dictionary by filtering pairs, which could be noise in the dictionary according to statistical information. Based on the seed dictionary we can learn the cross-lingual word embeddings using neural network trained with SGD. Further we use the mapping to infer the lexicon model for translation of next batch of sentences. In such close-loop process, mapping learning and sentence translation will be alternatively refined to an optimum state. In general, the efficiency of this online algorithm mainly depends on the corpus size and batch size of sentences, which decide directly the translation time. We can either extend the corpus size or loop over the same corpus more than once in order to enrich the data for this corpus-based approach.


\subsection{Training Details}
\begin{itemize}
	\item Dictionary Induction
	\begin{itemize}
		 With cross-lingual word embeddings, we can directly find the word translation using nearest neighbors search. The nearest neighbor suffers from the hubness problem. As mentioned in Section 3.4, we adopt the Cross-domain Similarity Local Scaling (CSLS) from \cite{conneau2017word}. Following the authors, we set $k=10$.
		 \item Frequency-based vocabulary cutoff\\
		 The size of the similarity matrix grows with respect to that of the target vocabulary. This does not increase the cost of computation, but it also makes the number of possible solutions grow rapidly. In the experiments we find out that those less frequent words can be noise for when inducing the word translation. And those noise translations would have global negative influence of the translation.
		 We propose to restrict the vocabulary size to the k-top frequent words in the source and target languages, where we found $k=50000$ can balance the efficiency and accuracy.	 
	\end{itemize}


	\item Corpus Translation
	Most words have multiple translations. Some are more likely than others. Also the translation depends on the context, some words are used only in certain circumstances. For example, 'home', 'house' are synonyms, but when translating 'zu Haus', we will choose 'home'.  With language model, we can choose better word translation candidates according to context words. 
	\begin{itemize}
		\item Language model score \\we use $5$-gram language model trained by kenlm tool.
		\item Lexicon score\\ we use simple linear mapping $q(f,e) = \frac{d(f,e)+1}{2}$, where $d(f,e)$ is the cosine similarity between $\bm{e}$ and $\bm{f}$	
		\item Scaling parameters for sub-models\\ 
		\[ \hat{e}_1^J = \argmax{e_1^J} \prod_{j=1}^{J} p^{\lambda_{LM}}(e_j| e_{j-4}^{j-1}) \cdot q^{\lambda_{lex}}(f_j, e_j)\]
		empirically set $\lambda_{lex}$ as 1 and  $\lambda_{LM}$ as 0.1
	\end{itemize}
	More details will be explained in Chapter 5.
	



	
	\item Optimization Objective \\
	Suppose we have got synthetic parallel sentences from translation and according to the alignment, we have extract word translation pairs as seed dictionary $dico$, the goal the optimization is to find the best mapping so that the lexicon induction can be generalized as much as possible. The linear mapping $W \in \mathbb{R}^{d \times d}$ will be trained on the pairs to minimize a measure of discrepancy between mapped  source word embeddings and the target word embeddings.
	\[ W = \argmin{W \in \mathbb{R}^{d \times d}} \frac{1}{n} \sum_1^n l(W\bm{f}_i, \bm{e}_i) \]
	where $l$ is a loss function, in this thesis, I compare the results of Euclidean distance loss and cosine similarity loss 
	\item Orthogonal Constraint\\
	As mentioned previously, the orthogonal preserves the quality of monolingual embeddings. It also preserves the the dot product of vectors and the $l_2$ distance. In our experiments, we find the orthogonal constraint make training procedure converge successfully, specially at the beginning stage. In this thesis, the orthogonal constraint is added during the training process; we use projected gradient descent to train the neural network by alternating the training and the orthogonal constraining:
\[ W \leftarrow (1+\beta) W - \beta(WW^\top)W\] 
	where $\beta$ = 0.01  is usually found to perform well.  This method ensures that the matrix stays close to the manifold of orthogonal matrices after each update. 	
	\item Learning Rate Scheduling\\
	There are two different learning rate schedules for our method. Once we get the batch sentences translation, we can either train the mapping with the learning rate inheriting from last training and then decrease the learning rate, or train always with the initial learning rate and keep training and update learning rate until the learning rate has dropped to the minimal learning rate we preset. The first training schedule fits the normal online algorithm principle best but in practice we find the second schedule trains the cross-lingual embedding more efficiently which may due to the translation task differs for different batches.
	\item Stop Criterion\\
	Once the results of this approach have converged to a good point, we can break the training loop early. Several different factors can be selected as the stop criterion such as the loss in the embedding training process or statistic on the translation. In practice, we find the model selection methods provided by \cite{conneau2017word} more suitable in our case. In more detail, they use CSLS retrieval to generate the translations of 10k most frequent source words. They then compute the average cosine similarity between these deemed translations and use this average as validation metric. Empirically, they found that this simple criterion is better correlated with the performance on the evaluation tasks than those distance based criterion. This metric can also be used for hyper-parameter selection.
\end{itemize}
%Our method can be combined with word-based machine translation. This approach can be further improved  with more word-based machine translation model. We can also refine the 