\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams et~al.(2017)Adams, Makarucha, Neubig, Bird, and
  Cohn]{adams2017cross}
Oliver Adams, Adam Makarucha, Graham Neubig, Steven Bird, and Trevor Cohn.
\newblock Cross-lingual word embeddings for low-resource language modeling.
\newblock In \emph{Proceedings of the 15th Conference of the European Chapter
  of the Association for Computational Linguistics: Volume 1, Long Papers},
  volume~1, pages 937--947, 2017.

\bibitem[Artetxe et~al.(2017{\natexlab{a}})Artetxe, Labaka, and
  Agirre]{artetxe2017learning}
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
\newblock Learning bilingual word embeddings with (almost) no bilingual data.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages
  451--462, 2017{\natexlab{a}}.

\bibitem[Artetxe et~al.(2017{\natexlab{b}})Artetxe, Labaka, Agirre, and
  Cho]{artetxe2017unsupervised}
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho.
\newblock Unsupervised neural machine translation.
\newblock \emph{arXiv preprint arXiv:1710.11041}, 2017{\natexlab{b}}.

\bibitem[Artetxe et~al.(2018)Artetxe, Labaka, and Agirre]{artetxe2018robust}
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
\newblock A robust self-learning method for fully unsupervised cross-lingual
  mappings of word embeddings.
\newblock \emph{arXiv preprint arXiv:1805.06297}, 2018.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Barone(2016)]{barone2016towards}
Antonio Valerio~Miceli Barone.
\newblock Towards cross-lingual distributed representations without parallel
  text trained with adversarial autoencoders.
\newblock \emph{arXiv preprint arXiv:1608.02996}, 2016.

\bibitem[Bojanowski et~al.(2016)Bojanowski, Grave, Joulin, and
  Mikolov]{bojanowski2016enriching}
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.
\newblock Enriching word vectors with subword information.
\newblock \emph{arXiv preprint arXiv:1607.04606}, 2016.

\bibitem[Cao et~al.(2016)Cao, Zhao, Zhang, and Meng]{cao2016distribution}
Hailong Cao, Tiejun Zhao, Shu Zhang, and Yao Meng.
\newblock A distribution-based model to learn bilingual word embeddings.
\newblock In \emph{Proceedings of COLING 2016, the 26th International
  Conference on Computational Linguistics: Technical Papers}, pages 1818--1827,
  2016.

\bibitem[Cheng et~al.(2016)Cheng, Xu, He, He, Wu, Sun, and Liu]{cheng2016semi}
Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
\newblock Semi-supervised learning for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1606.04596}, 2016.

\bibitem[Cohn and Lapata(2007)]{cohn2007machine}
Trevor Cohn and Mirella Lapata.
\newblock Machine translation by triangulation: Making effective use of
  multi-parallel corpora.
\newblock In \emph{Proceedings of the 45th Annual Meeting of the Association of
  Computational Linguistics}, pages 728--735, 2007.

\bibitem[Conneau et~al.(2017)Conneau, Lample, Ranzato, Denoyer, and
  J{\'e}gou]{conneau2017word}
Alexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and
  Herv{\'e} J{\'e}gou.
\newblock Word translation without parallel data.
\newblock \emph{arXiv preprint arXiv:1710.04087}, 2017.

\bibitem[Coulmance et~al.(2016)Coulmance, Marty, Wenzek, and
  Benhalloum]{coulmance2016trans}
Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, and Amine Benhalloum.
\newblock Trans-gram, fast cross-lingual word-embeddings.
\newblock \emph{arXiv preprint arXiv:1601.02502}, 2016.

\bibitem[Dhillon et~al.(2011)Dhillon, Foster, and Ungar]{dhillon2011multi}
Paramveer Dhillon, Dean~P Foster, and Lyle~H Ungar.
\newblock Multi-view learning of word embeddings via cca.
\newblock In \emph{Advances in neural information processing systems}, pages
  199--207, 2011.

\bibitem[Duong et~al.(2016)Duong, Kanayama, Ma, Bird, and
  Cohn]{duong2016learning}
Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, and Trevor Cohn.
\newblock Learning crosslingual word embeddings without bilingual corpora.
\newblock \emph{arXiv preprint arXiv:1606.09403}, 2016.

\bibitem[Faruqui and Dyer(2014)]{faruqui2014improving}
Manaal Faruqui and Chris Dyer.
\newblock Improving vector space word representations using multilingual
  correlation.
\newblock In \emph{Proceedings of the 14th Conference of the European Chapter
  of the Association for Computational Linguistics}, pages 462--471, 2014.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{gehring2017convolutional}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock \emph{arXiv preprint arXiv:1705.03122}, 2017.

\bibitem[Gouws et~al.(2015)Gouws, Bengio, and Corrado]{gouws2015bilbowa}
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
\newblock Bilbowa: Fast bilingual distributed representations without word
  alignments.
\newblock In \emph{International Conference on Machine Learning}, pages
  748--756, 2015.

\bibitem[Hauer et~al.(2017)Hauer, Nicolai, and Kondrak]{hauer2017bootstrapping}
Bradley Hauer, Garrett Nicolai, and Grzegorz Kondrak.
\newblock Bootstrapping unsupervised bilingual lexicon induction.
\newblock In \emph{Proceedings of the 15th Conference of the European Chapter
  of the Association for Computational Linguistics: Volume 2, Short Papers},
  volume~2, pages 619--624, 2017.

\bibitem[He et~al.(2016)He, Xia, Qin, Wang, Yu, Liu, and Ma]{he2016dual}
Di~He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying
  Ma.
\newblock Dual learning for machine translation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  820--828, 2016.

\bibitem[Heafield(2011)]{heafield2011kenlm}
Kenneth Heafield.
\newblock Kenlm: Faster and smaller language model queries.
\newblock In \emph{Proceedings of the Sixth Workshop on Statistical Machine
  Translation}, pages 187--197. Association for Computational Linguistics,
  2011.

\bibitem[Kim et~al.(2017)Kim, Schamper, and Ney]{kim2017unsupervised}
Yunsu Kim, Julian Schamper, and Hermann Ney.
\newblock Unsupervised training for large vocabulary translation using sparse
  lexicon and word classes.
\newblock In \emph{Proceedings of the 15th Conference of the European Chapter
  of the Association for Computational Linguistics: Volume 2, Short Papers},
  volume~2, pages 650--656, 2017.

\bibitem[Koehn(2009)]{koehn2009statistical}
Philipp Koehn.
\newblock \emph{Statistical machine translation}.
\newblock Cambridge University Press, 2009.

\bibitem[Lample et~al.(2017)Lample, Denoyer, and
  Ranzato]{lample2017unsupervised}
Guillaume Lample, Ludovic Denoyer, and Marc'Aurelio Ranzato.
\newblock Unsupervised machine translation using monolingual corpora only.
\newblock \emph{arXiv preprint arXiv:1711.00043}, 2017.

\bibitem[Lample et~al.(2018)Lample, Ott, Conneau, Denoyer, and
  Ranzato]{lample2018phrase}
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio
  Ranzato.
\newblock Phrase-based \& neural unsupervised machine translation.
\newblock \emph{arXiv preprint arXiv:1804.07755}, 2018.

\bibitem[Lu et~al.(2015)Lu, Wang, Bansal, Gimpel, and Livescu]{lu2015deep}
Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel, and Karen Livescu.
\newblock Deep multilingual correlation for improved word embeddings.
\newblock In \emph{Proceedings of the 2015 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 250--256, 2015.

\bibitem[Luong et~al.(2015{\natexlab{a}})Luong, Pham, and
  Manning]{luong2015effective}
Minh-Thang Luong, Hieu Pham, and Christopher~D Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock \emph{arXiv preprint arXiv:1508.04025}, 2015{\natexlab{a}}.

\bibitem[Luong et~al.(2015{\natexlab{b}})Luong, Pham, and
  Manning]{luong2015bilingual}
Thang Luong, Hieu Pham, and Christopher~D Manning.
\newblock Bilingual word representations with monolingual quality in mind.
\newblock In \emph{Proceedings of the 1st Workshop on Vector Space Modeling for
  Natural Language Processing}, pages 151--159, 2015{\natexlab{b}}.

\bibitem[Mikolov et~al.(2013{\natexlab{a}})Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013{\natexlab{a}}.

\bibitem[Mikolov et~al.(2013{\natexlab{b}})Mikolov, Le, and
  Sutskever]{mikolov2013exploiting}
Tomas Mikolov, Quoc~V Le, and Ilya Sutskever.
\newblock Exploiting similarities among languages for machine translation.
\newblock \emph{arXiv preprint arXiv:1309.4168}, 2013{\natexlab{b}}.

\bibitem[Mikolov et~al.(2013{\natexlab{c}})Mikolov, Sutskever, Chen, Corrado,
  and Dean]{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In \emph{Advances in neural information processing systems}, pages
  3111--3119, 2013{\natexlab{c}}.

\bibitem[Neishi et~al.(2017)Neishi, Sakuma, Tohda, Ishiwatari, Yoshinaga, and
  Toyoda]{neishi2017bag}
Masato Neishi, Jin Sakuma, Satoshi Tohda, Shonosuke Ishiwatari, Naoki
  Yoshinaga, and Masashi Toyoda.
\newblock A bag of useful tricks for practical neural machine translation:
  Embedding layer initialization and large batch size.
\newblock In \emph{Proceedings of the 4th Workshop on Asian Translation
  (WAT2017)}, pages 99--109, 2017.

\bibitem[Nuhn and Ney(2014)]{nuhn2014decipherment}
Malte Nuhn and Hermann Ney.
\newblock Em decipherment for large vocabularies.
\newblock In \emph{Proceedings of the 52nd Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, volume~2, pages
  759--764, 2014.

\bibitem[Nuhn et~al.(2012)Nuhn, Mauser, and Ney]{nuhn2012deciphering}
Malte Nuhn, Arne Mauser, and Hermann Ney.
\newblock Deciphering foreign language by combining language models and context
  vectors.
\newblock In \emph{Proceedings of the 50th Annual Meeting of the Association
  for Computational Linguistics: Long Papers-Volume 1}, pages 156--164.
  Association for Computational Linguistics, 2012.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 1532--1543, 2014.

\bibitem[Qi et~al.(2018)Qi, Sachan, Felix, Padmanabhan, and Neubig]{qi2018and}
Ye~Qi, Devendra~Singh Sachan, Matthieu Felix, Sarguna~Janani Padmanabhan, and
  Graham Neubig.
\newblock When and why are pre-trained word embeddings useful for neural
  machine translation?
\newblock \emph{arXiv preprint arXiv:1804.06323}, 2018.

\bibitem[Ravi and Knight(2011)]{ravi2011deciphering}
Sujith Ravi and Kevin Knight.
\newblock Deciphering foreign language.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies-Volume 1}, pages
  12--21. Association for Computational Linguistics, 2011.

\bibitem[Ruder et~al.(2017)Ruder, Vuli{\'c}, and S{\o}gaard]{ruder2017survey}
Sebastian Ruder, Ivan Vuli{\'c}, and Anders S{\o}gaard.
\newblock A survey of cross-lingual word embedding models.
\newblock \emph{arXiv preprint arXiv:1706.04902}, 2017.

\bibitem[Smith et~al.(2017)Smith, Turban, Hamblin, and
  Hammerla]{smith2017offline}
Samuel~L Smith, David~HP Turban, Steven Hamblin, and Nils~Y Hammerla.
\newblock Offline bilingual word vectors, orthogonal transformations and the
  inverted softmax.
\newblock \emph{arXiv preprint arXiv:1702.03859}, 2017.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[Xing et~al.(2015)Xing, Wang, Liu, and Lin]{xing2015normalized}
Chao Xing, Dong Wang, Chao Liu, and Yiye Lin.
\newblock Normalized word embedding and orthogonal transform for bilingual word
  translation.
\newblock In \emph{Proceedings of the 2015 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1006--1011, 2015.

\bibitem[Zhang et~al.(2017)Zhang, Liu, Luan, and Sun]{zhang2017adversarial}
Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun.
\newblock Adversarial training for unsupervised bilingual lexicon induction.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages
  1959--1970, 2017.

\end{thebibliography}
