\chapter{Machine Translation}
\section{Rule-based machine translation}


Rule-based machine translation is machine translation system based on linguistic information. An RBMT system generates translation based on different levels of analysis, e.g. part of speech tagging (POS), morphological analysis,  semantic analysis, constituent analysis, dependency analysis.

Rule-based machine translation has the following advantages: No bilingual texts are required. Also because RBMT are built on a source language analysis and the target language generator and the source analysis part and target generation part are seperate, so it can be shared between multiple translation system if we replace the part with an other part for a closed related language.
However the method is still not so general, we need to build the dictionary and  linguistic rule set manually, it is very expensive. also it is hard to deal with rule interactions in big systems, ambiguity and idiomatic expression.




\section{Statistical machine translation}
The initial models for machine translation are based on words as units (Word-based machine translation), that can be translated, inserted, dropped and reordered.
Fertility is the notion that input words produce a specific number of output words in the output language.

Define the phrase-based statistical machine translation model mathematically.  First apply the Bayes rule to invert the translation direction and integrate a language model $p_{LM}$ so the best English translation for the input sentence ${f} $ is defined as 

The advantages of the phrase-based machine translation is :
\begin{enumerate}
	\item many-to-many translation can handle non-compositional phrase	
	\item better utilization of local context in translation
	\item the more data, the longer phrases can be learned 
\end{enumerate}

\subsubsection{Probabilistic Model}
Bayes rule \\
%\begin{align}
%	\bm{e_{best}} & = \argmax_{\bm{e}} {p(\bm{e} | \bm{f})} \\
%	& = \argmax_{\bm{e}} {p(\bm{f|e}) p_{LM}(\bm{f} | \bm{e})}
%\end{align}
translation model ${p(\bm{e}| \bm{f})}$, language model ${p_{LM}(\bm{e})}$

\subsubsection{Weighted Model}
Describe standard model consists of three sub-models:
\begin{enumerate}
\item phrase translation model 
\item reordering model ${d}$
\item language model ${p_{LM}(e)}$
\end{enumerate}
Phrase translation model can be learned based on a  word alignment, extraction of phrase pairs and scoring phrase pairs or using the EM algorithm to learn the phrase table
Reordering model can be distance based reordering model or lexicalized reordering, the lexicalized reordering model predicts the orientation of a phrase: either monotone, discontinuous or swap word alignment, phrase extraction, phrase scoring align phrase pairs directly with EM algorithm
Add weights: ${\lambda_{\Phi}}$, ${\lambda_{d}}$ , ${\lambda_{LM}}$


\subsubsection{ Log-linear Model Combination}
\[p(e, a | f) =exp[\lambda \sum_{i=1}^{I} \log \Phi(f_i|)] \]
Reordering is handled by a distance-based reordering model.

Different mode components in the phrase model are combined in a log-linear model, in which each component is a factor which maybe be weighted. The model cab be further extended by components like: bidirectional translation probabilities, lexical weighting, word penalty and phrase
\section{Neural machine translation}
Unlike traditional phrase-based machine translation, which consists of several models that are tuned separately, neural machine translation tries to build a more general neural network model which can directly output translations given input.
The most common network structure is the encoder-decoder framework and 
\[ p(e_1^I | f_1^J) = \prod_{t} p(e_i|e_0^{t-1}, f_1^J) = \prod_{t} {p(e_t| e_{t-1}, h_{t-1}, f_1^J)} \] 
extended language model for target word sequence

\[ p(e_t|e_{t-1} h_{t-1}, f_1^J) =  p(e_t| e_{t-1}, h_{t-1}, c_t)\]
\[ \bm{\tilde{h_{t}}} = tanh(\bm{W_{c}[c_t; h_t]}) \]
\[p(y_t| y_{<t}, x) = softmax(\bm{W_s \tilde{h_t}})\]


\section{Unsupervised Machine Translation}
They initialize the model with an inferred bilingual dictionary. They leverage strong language model: denoising autoencoder,  third, they implemented the back translation: the key idea is to train two translation models which translate in contrary directions at the same time. The last property is that the models constrain the latent representations produced by the encoder to be shared between the two languages. The encoders will encoder the input into a common latent representation space independent of the language. The decoder plays the role of translator and will try to learn to improve the translation quality with the help of back translation mechanism.

\subsection{Language Model}

\subsection{Back Translation}

\subsection{Shared Latent Representations}





































