\chapter{Machine Translation}
This chapter describes the classical or start-of-the-art machine translation models from statistical machine translation model to neural machine model. The relative research works on unsupervised machine translation will also be included.


\section{Statistical Machine Translation}
%%%
%The initial models for machine translation are based on words as units (Word-based machine translation), that can be translated, inserted, dropped and reordered.
%Fertility is the notion that input words produce a specific number of output words in the output language.
%
%Define the phrase-based statistical machine translation model mathematically.  First apply the Bayes rule to invert the translation direction and integrate a language model $p_{LM}$ so the best English translation for the input sentence ${f} $ is defined as 
%
%The advantages of the phrase-based machine translation is :
%\begin{enumerate}
%	\item many-to-many translation can handle non-compositional phrase	
%	\item better utilization of local context in translation
%	\item the more data, the longer phrases can be learned 
%\end{enumerate}
Statistical machine translation has achieved success until the beginning of this century. The initial statistical models for machine translation are based on words as atomic units that may be translated, inserted, dropped and reordered. In statistical machine translation, we use both a translation model and a language model which ensures fluent output. Later the statistical machine translation prefers to use translation of phrases as atomic units. These phrases are any contiguous sequences of words, not necessarily linguistic entities. In this approach, the input sentence is broken up into a sequence of phrases, these phrases are mapped one-to-one to output phrases, which may be reordered.

\subsection{Word-Based Model}
\noindent \textbf{Noisy-channel model:}
Noisy channel model based on the notion of a noisy channel from Shannon's information theory has been applied to many language processing problems. Assume the source sentence is a distorted message omitted from the target sentence, we have a model on  how the message is distorted (translation model $Pr(f_1^J|e_1^I)$) and also a model on which original messages are probable (language model ${Pr(e_1^I)}$), our task is to find the best translation ${e_1^I}$ for an input foreign sentence.
\begin{align*}
\argmax{e_1^I} \{Pr(e_1^I | f_1^J)\} & = \argmax{e_1^I} {\frac{Pr(f_1^J | e_1^I)Pr(e_1^I)}{Pr(f_1^J)}} \\
& = \argmax{e_1^I} { \{ Pr(f_1^J | e_1^I) \cdot  Pr(e_1^I)\} }
\end{align*} 


The basic model for word-based model is noisy-channel model. The task of word alignment is an artifact of word-based machine translation. Alignment models target at the reordering problem for word-based translation.\\

\subsubsection{IBM Models}
Based on the noisy-channel model, IBM word-based translation make the model more complicated by adding submodels. Starting from lexical translation, absolute alignment model, fertility etc. are added step by step.\\

 \textbf{Alignment model} introduce an explicit model for reordering words in a sentence. More often than not, words that follow each other in one language have translations that follow each other in one language have translations that follow each other in the output language. In detail, the position $j$ in the source sentence is aligned with the position $i$ in the target sentence when translating, denoted as  ${i=a_j}$. Alignment model is a global reordering model.For whole sentence, we denote the alignment as 
\[a_1^J:= a_1\cdots a_J\]

\textbf{Fertility model} the specific number of output words in the output language. Generally one word in one language just translate into one single word in the other language. But some words produce multiple words or get dropped. we denote the fertility model as $\phi(e)$, so the length of translation sentence  
\[ J = \sum_{i=0}^{I} \phi(e_i) \]

The word-based translation process can be described as the following figure, we use IBM-3 model for example:  
IBM-3 model contains four steps: 
\begin{itemize}
	\item Fertility step
	\item NULL insertion
	\item Lexical translation step
	\item Distortion step for reordering
\end{itemize}
\begin{figure}[t]
	\includegraphics[width=12cm]{wordsmt}
	\caption{ Illustration of IBM-3 model (\cite{koehn2009statistical})}
	\centering
\end{figure}




\subsubsection{Weighted Model}
To overcome shortcomings in modeling, introduce scaling exponents ${\lambda_{m}}$ as in speech recognition. For example
\[ Q(f_1^J, e_1^I; a_1^J) = P(J|I)^{\lambda_1} \cdot \prod_{i} P(e_i|e_h)^{\lambda_2} \cdot \prod_{j} [p(a_j|a_{j-1}, I, J)^{\lambda_{3}} \cdot p(f_j|e_{a_j})^{\lambda_4}]  \]
where the four probabilities corresponds to length model, language model, alignment model and lexical model separately. $e_h$ denotes the history words in $N$-gram model



\subsection{Phrase-Based Model}
Actually when translating, words may not be the best candidates for the smallest units for translation, sometimes one word in a foreign languages should be translated into two English words, or vice versa. Word-based models often break down in these cases.\\
Phrase-based models typically do not strictly follow the noisy-channel approach proposed for word-based models, but use a log-linear framework This model allow s straightforward integration if additional features.\\

\textbf{Log-linear Model Combination}:
Consider arbitrary models ("feature functions"):
\[ q_m(f_1^J, e_1^I; a_1^J) > 0  \quad m = 1, \cdots, M\]

\begin{align*}
	Q(e_1^I, f_1^J; a_1^J) & = \prod_{m=1}^{M} q_m(f_1^J, e_1^I; a_1^J)^{\lambda_m} \\
	& = \exp(\sum_{m=1}^{M} \lambda_m \log q_m(f_1^J, e_1^I; a_1^J))
\end{align*}
In this frame work, we view each data point as a vector of features and the model as a set of corresponding feature functions, these functions are trained separately and combined assuming that they are independent of each other.\\ 

Components for log-linear model can be such as language model, phrase translation model, reordering model are used as feature functions with appropriate weights.
\begin{itemize}
	\item Phrase translation model can be learned from a word-aligned parallel corpus, alternatively we also can use expectation maximization algorithm to directly find phrase alignment for sentence pairs.
	\item Reordering model for phrase case is typically modeled by a distance based reordering cost that discourage reordering in general. Lexicalized reordering model can be introduced for specific phrase pair.
\end{itemize}

The model can be further extended by components like: bidirectional translation probabilities, lexical weighting, word penalty and phrase penalty. 

\subsection{Decipherment}
\cite{ravi2011deciphering} frame the MT problem as a decipherment task, treating the foreign text as a cipher. They also propose iterative expectation-maximization method  (EM) and Bayesian decipherment to train this unsupervised model. Many concepts in deciperment are the same as the SMT.\\

IBM model tries to maximize the probability with hidden alignment model
\[ \argmax{\theta} \prod_{\bm{e}, \bm{f}} {P_{\theta}(\bm{f}|\bm{e})} = \argmax{\theta} \prod_{\bm{e}, \bm{f}} \sum_{a} P_{\theta}(\bm{f}, a| \bm{e})  \]
While for unsupervised case, we train 
\[ \argmax{\theta} \prod_{\bm{f}}P_{\theta}(\bm{f})= \argmax{\theta} \prod_{\bm{f}} \sum_{\bm{e}} P(\bm{e}) \cdot P_{\theta}(\bm{f}|\bm{e})\]

for hidden alignments: \[ \argmax{\theta} \prod_{\bm{f}} \sum_{\bm{e}} P(\bm{e}) \cdot \sum_{a} P_{\theta}(\bm{e},a|\bm{e}) \]

Since the model is very complicated, more assumptions are added to the model. The model accounts for word substitutions, insertions, deletions and local reordering during the translation process but does not incorporate fertilities or global re-ordering.\\

The generative process:
\begin{enumerate}
	\item Generate an English sentence $\bm{e}$ with probability $P(\bm{e})$.
	\item Insert a NULL word at any position in the English sentence with uniform probability.
	\item For each English word token $e_i$ (including NULLs), choose a foreign word translation $f_i$, with probability $P_{\theta}(f_i| e_i)$, the foreign word may be NULL.
	\item Swap any pair of adjacent foreign words $f_{i-1}, f_i$, with probability ${P_{\theta}(swap)}$. 
	\item Output the foreign string $f_1^M$, skipping over NULLs.
\end{enumerate}

However, this method is limited to rather short sentences and simplistic settings.
\section{Neural Machine Translation}
Neural machine translation (NMT) has recently become the dominant method for machine translation task. In comparison to the traditional statistical machine translation (SMT), NMT systems are trained end-to-end, taking advantages of the continuous representation of the hidden states that greatly alleviate the sparsity problem and make better use of more contextual information. Besides this, traditional phrase-based machine translation, which consists of several models that are tuned separately, neural machine translation tries to build a more general neural network model which can directly output translations given input, it contains only one single model, and only one single training criterion. \\
The first successful neural machine translation, attention mechanism has lately been used to improve neural machine translation by selectively focusing on parts of the source sentence during translation. The inherently sequential nature precludes parallelization within training examples. Convolutinoal sequential to sequence (ConvSeq2seq) and Transformer architectures are brought forward  for significantly more parallelization during training to better exploit the GPU hardware and these models can reach a new state of the art in
translation quality.


\subsection{Basic Model}
\begin{figure}[t]
	\includegraphics[width=12cm]{nmt}
	\caption{ Neural machine translation â€“ example of a deep recurrent architecture (\cite{luong2015effective})}
	\centering
\end{figure}

The most common network structure is the encoder-decoder framework, in which two recurrent neural networks work together to transform one sequence to another. An encoder condenses an input sequence into a vector, and a decoder unfolds that vector into a new sequence. The model can be expressed as:
\[ p(e_1^I | f_1^J) = \prod_{t} p(e_i|e_0^{i-1}, f_1^J) = \prod_{i} {p(e_i|e_0^{i-1},  {\bm{s}})} \] 
Since the decoder is actually a RNN structure, the conditional probability can be simplified as:
\[ p(e_i|e_0^{i-1}, \bm{s}) = p(e_i| e_{i-1}, \bm{h}_{i-1}, \bm{s}) \]
where $\bm{h}_{i-1}$ denotes the hidden state of decoder and $\bm{s}$ is the dense vector representation of source sentence.
In more detail, the probability of decoding each word $e_i$ as:
\[ p(e_i| e_{i-1}, \bm{h}_{i-1}, {\bm{s}}) = softmax(g(\bm{h_i}))\]
$g$ is trainable function which gives the probability of all target words in the vocabulary. $\bm{h}_i$ is the RNN hidden unit, abstractly computed as:
\[ \bm{h_i} = f(\bm{h}_{i-1}, \bm{s})\]

 $f$ defines the transformation connecting hidden states.  \\








%
%\[ p(e_t|e_{t-1} h_{t-1}, f_1^J) =  p(e_t| e_{t-1}, h_{t-1}, c_t)\]
%
%\[ \bm{\tilde{h_{t}}} = tanh(\bm{W_{c}[c_t; h_t]}) \]
%\[p(y_t| y_{<t}, x) = \text{softmax}(\bm{W_s \tilde{h_t}})\]

Drawbacks of the such seq2seq model:
\begin{enumerate}
	\item The model compresses all information from the input sentence into a hidden vector ${1}$, while ignores the length of input sentence, when the length of input sentence get very long, even longer than the training sentences, it becomes harder to extract specific information for predicting the target word, the performance will get worse.
	\item It's not suitable to assign the same weight to all input words, one target word corresponds usually to one or several words in the input sentence. Treating all words equally does not distinguish the source information and influence the performance badly.
\end{enumerate}

\subsection{Attention Mechanism}
To solve the problem mentioned above, attention mechanism was proposed to derive a context vector ${\bm{c_i}}$ that capture the input information to help to predict the target word at the position ${i}$. The basic idea is: given the target hidden state ${\bm{h_i}}$ and the source-side context vector $\bm{c_t}$, we can compute the hidden state ${\tilde{\bm{h}}_i}$ by combining the current hidden state $\bm{h_i}$ and the context vector $\bm{c_i}$:
\[ \tilde{\bm{h}}_i = tanh(W_c[\bm{c_i}; \bm{h_i}])\]

Then the target word can be predicted by softmax function:
\[  p(e_i|e_0^{i-1}, f_1^J) = softmax(W_s \tilde{\bm{h}}_t)\] 
\begin{figure}
	\begin{minipage}[h]{0.5\textwidth}
		\includegraphics[width=\textwidth]{attentiong}
		\caption{Picture 1}
		\label{fig:1}
	\end{minipage}
	
	\begin{minipage}[h]{0.5\textwidth}
		includegraphics[width=textwidth]{attentionl}
		\caption{Picture 2}
		\label{fig:2}
	\end{minipage}
\end{figure}

%\begin{figure}[t]
%	\includegraphics[width=6cm]{attentiong}
%	\caption{Global attention model (\cite{luong2015effective})}
%	\centering
%\end{figure}
%
%\begin{figure}[t]
%	\includegraphics[width=6cm]{attentionl}
%	\caption{Local attention model (\cite{luong2015effective})}
%	\centering
%\end{figure}
The concept of attention mechanism comes first from the computer vision domain, when generating image captions,  The model learns f to restrict attention to particular objects in the image.
We denote the decoder RNN state as $\bm{s}_i$, the encoder hidden state over all position as $\bm{h}_j$.\\

\textbf{Global attention \& Local attention } \\
The global attention attend all the input words, weighted average of source hidden states (word representations):
\[ \bm{c}_i = \sum_{j} \bm{\alpha}_i(j)\cdot  \bm{h}_j \]
where $\bm{\alpha}_i(j)$ is the normalized attention weight that output at position $i$ is aligned with input position $j$, and it can be calculated as the following softmax like function:
\begin{align}
\bm{\alpha}_i(j) = & \ \text{align}(\bm{s}_i, {\bm{h}}_j) \\
= & \ \frac{\text{exp\ }(\text{score \cite{xu2015show}}(\bm{s}_i, {\bm{h}}_j))}{\sum_{\bm{j}^{\prime}} \text{exp\ }(\text{score}(\bm{s}_i, {\bm{h}}_j^{\prime}))}
\end{align}
Since score function is referred as a content based function, different forms can be considered:
\begin{equation}
\text{score}(\bm{s_i}, {\bm{h}}_j)=\left\{
\begin{array}{lcl}
{\bm{s_i}}^T {\bm{h}}_j & & dot\\
{\bm{s_i}}^T W_a {\bm{h}}_j & & general\\
{\bm{v_i}}^T tanh(W_a[\bm{s_i}; {\bm{h}}_j]) & & concat
\end{array} \right.
\end{equation}



Since for global attention, for each target word we need to attend the whole input sentence, it is very expensive and impractical to translate longer sentences. \cite{luong2015effective}) proposed the local attention, the local attention is actually the  trade-off between soft and hard attention. \\
For local attention, the model first generates an aligned position ${a(i)}$ for each target word at position $i$. Then the context vector ${\bm{c}_i}$ is a weighted sum within the window ${[a(i)-D, a(i)+D]}$, ${D}$ is selected empirically. The model predict the aligned  position ${p_i}$ as followed:
\[ a(i) = S \cdot \text{sigmoid}(\bm{v}_p^T \text{tanh}(W_p \bm{h}_i))\]

${W_p}$ and ${\bm{v}_p}$ are the model parameters which will be learned to predict positions. $S$ is the source sentence length. As a result of sigmoid function, $a(i) \in [0, S]$. To favor the words near position ${a(i)}$, they place a Gaussian distribution which centered at ${a(i)}$.
\[\bm{\alpha}_i(j) = \text{align}(\bm{s}_i, {\bm{h}}_j) \cdot \text{exp\ }\Big(-\frac{(j-a(i))^2}{2 \sigma^2}\Big) \]

\subsection{Transformer}
\begin{figure}[t]
	\includegraphics[width=10cm]{transformer}
	\caption{The Transformer model architecture (\cite{vaswani2017attention})}
	\centering
\end{figure}
The RNN encoder-decoder models have achieved state of the art in sequence modeling and machine translation problem. However such RNN models also have some disadvantages, because of their inherently sequential computation which prevents parallelization across elements of the input sequence. That means it is more difficult to fully take advantage of the modern computing devices like GPU and TPU. Convolutional \cite{gehring2017convolutional} and fully-attentional feed-forward architectures like Transformer \cite{vaswani2017attention} models are proposed as alternatives for RNNs. \\

\textbf{Dot-Product Attention}
Given a query $\bm{q}$ and a set of key-value $(\bm{k}-\bm{v})$ pairs, the output is weighted average of values, where the weight of each value is computed by inner product of query and corresponding key. The queries and keys are of the same dimension ${d_k}$, values are of dimension ${d_v}$. Actually, for better understanding, the query $\bm{q}$  can be considered as the decoder state $\bm{s}_i$ and key $\bm{k}$ and values $\bm{v}$ can be considered as the encoder state $\bm{h}_j$ in previous attention model. Then the attention of query $\bm{q}$ on all $(\bm{k}-\bm{v})$ pairs are:
\[ A(\bm{q}, K, V) = \sum_{i}{ \frac{\exp\ ({\bm{q}\cdot \bm{k}_i})}{\sum_{j} \exp\ {\bm{q}\cdot \bm{k}_j} } \cdot \bm{v}_i}\]
When we stack the queries ${\bm{q}}$ to ${Q}$:
\[ A(Q, K, V) = \text{softmax}(QK^T)V\]

As ${d_k}$ get larger, the variance of ${q^T k}$ get larger.  The softmax become very peaked and the gradient get smaller. To counteract this effect, we scaled the dot products by ${\frac{1}{\sqrt{d_k}}}$, we get

\[ Attention(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]
The shape of resulting matrix is $n_q \times d_v$\\

\textbf{Multi-head attention}\\
 First map $Q$, $K$, $V$ into $h$ many lower dimension spaces via linear mapping matrix. Here $h$ is the number of heads. Then apply attention and concatenate the outputs. \\
Suppose the original dimensions of queries, keys, values are ${d_{m}}$. We the number of heads is ${h}$, then we set ${d_k = d_v = d_{m}/h}$. With ${i \in 1 \cdots h}$ different matrix ${W_i^Q \in R^{d_{m}\times d_k}}$ ${W_i^K \in R^{d_{m}\times d_k}}$ ${W_i^V \in R^{d_{m}\times d_k}}$ and ${W^O \in R^{d_{m}\times d_k}}$.

\[ \text{MultiHead}(Q, K, V) = Concat(h\text{head}_1, \cdots, \text{head}_h)W^O \]
\[\text{head}_i = Attention(QW_i^Q, KW_i^K, VW_i^V) \]

where  $\text{head}_i \in \mathbb{R}^{n_q \times d_k}$,  ${\text{Multihead}(Q,K,V) \in \mathbb{R}^{n_q \times d_m}}$\\


There are three attention types in the transformer model:
\begin{itemize}
	\item encoder-decoder attention\\
	the queries come from the previous decoder layer, the keys and values are just the output of decoder. This works as the attention mechanism in the seq2seq model, it allow the decoder to align all positions in the input sequence with different weights
	\item encoder self-attention\\
	all queries, keys, values come from the encoder layer, each position allows to attend to all positions before that position
	\item decoder self-attention\\
	similar to encoder attention, each position is allowed to attend to position before and including that position
\end{itemize}
.


\textbf{Positional Encoding}\\
Since there is no RNN or CNN structures in transformer model, we need also need to make use of sequence information for seq2seq learning. We need inject position information into the embedding.
\[PE_{(\text{pos}, 2i)} = sin(\text{pos}/ 10000 ^{2i / d_{m}})\]
\[ PE_{(\text{pos}, 2i+1)} = cos(\text{pos} / 10000^{2i/d_{m}})\]


where pos is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponding to a sinusoid.

\section{Unsupervised Machine Translation}
give the first results for training a full translation model from non-parallel data, and use this model to translate previously-unseen text. The proposed problem named "deciperment" has received a lot of attention. This model contains many statistical methods, like EM algorithm or Bayesian method. However, this method is limited to rather short sentences and simplistic setting. With the development of neural machine translation, the end-to-end method performs better and easier to tune without much specific linguistic knowledge. The Idea is similar to dual learning framework except that in dual learning model, gradients are backpropagated through the reverse model and pretrain using a relative large parallel data as a warm start. The neural model mapping the sentences from monolingual corpora into the same latent space, By learning to reconstruct in both languages from the shared feature space. The translation model in both direction will be improved synchronously with the back-translation.




\subsection{Neural Unsupervised Machine Translation}

\begin{figure}[h]
	\includegraphics[width=14cm]{unmt}
	\caption{Illustration of unsupervised machine translation: A) the two original monolingual data; B) Initialization, the two distribution are roughly aligned; C) Denoising autoencoder, make the distribution closer to normal one in corresponding language D) Back-translation, improve the both translation model in iterative way (\cite{lample2018phrase})}

\end{figure}

%The key idea is to build a common latent space between the two languages (or domains) and to learn to translate by reconstructing in both domains according to two principles. 



%They initialize the model with an inferred bilingual dictionary. They leverage strong language model: denoising autoencoder,  third, they implemented the back translation: the key idea is to train two translation models which translate in contrary directions at the same time. The last property is that the models constrain the latent representations produced by the encoder to be shared between the two languages. The encoders will encoder the input into a common latent representation space independent of the language. The decoder plays the role of translator and will try to learn to improve the translation quality with the help of back translation mechanism.

\textbf{Dual Structure}\\
Dual structure is inspired by the observation: any machine translation has a dual task: e.g. German-to-English translation (primal) and English-to-German translation (dual). Dual tasks can form a closed loop and generate informative  feedback signals to for both translation models. Based on the feedback signals generated during this process, and leverage language model, we can minimize the reconstruction error of the original sentences. We can iteratively update the two models until convergence.   \cite{he2016dual} trained two agents to translate in opposite directions (e.g. French $\rightarrow$ English and English $\rightarrow$ French), and make them teach each other through a reinforcement learning process. While promising, this approach still requires a parallel corpus for a warm start. \cite{lample2018phrase} simplified the structure, in the proposed model, the gradients will not be back-propagated through the reverse model, but only make use of the back-translation. The training process as followed:

\begin{algorithm}[H]
	\SetAlgoLined
	\textbf{Language models:} Learn language models $P_s$, $P_t$ over source and target languages;\\
	\textbf{Initial translation model:}  Leveraging $P_s$, $P_t$, lean two initial translation models in each direction: $P_{s\rightarrow t}^{(0)}$, ${P_{t \rightarrow s}^{(0)}}$;\\
	\For{k = 1 to N}{
		\textbf{Back-translation:} Generate source and target sentences using the current translation models $P_{t\rightarrow s}^{(k-1)}$ and $P_{s\rightarrow t}^{(k-1)}$, leverading $P_s$, $P_t$;\\
		\textbf{Retrain:} Train new translation models $P_{t\rightarrow s}^{(k)}$ and $P_{s\rightarrow t}^{(k)}$ using the generated sentences and leverading $P_s$, $P_t$;
	}
	\caption{Unsupervised Machine Translation}
\end{algorithm}



\textbf{Initialization}\\
If without enough information to start the dual machine translation system, it will be hard for the system to catch the meaningful signals and then it will take much more iterations. However if we initialize the system by a naive a word-by-word translation of the sentence, where the bilingual lexicon are derived from the same monolingual data. While such initial "word-by-word" translation maybe poor if languages or corpora are not closely related, it still preserves some of the original semantics. Such word-by-word translation can already achieve several BLEU scores.\\
%\log P_{t\rightarrow t}(e|\text{noise}(e)
%\log P_{s\rightarrow s}(f|\text{noise}(f)) 

\textbf{Shared Latent Representation} \\
A shared encoder representation actis like an interlingua, which is translated in the decoder corresponding language regardless less of the input source language. Since the only supervision information only comes from the monolingual data. The model learn to translate by learning to reconstruct in both language from this shared space.\\
There are two different methods to learn the shared feature space:
\begin{itemize}
	\item Shared encoder
	The system use only one encoder that is shared by both languages involved. The universal encoder is aimed to produce a language independent representation of the input language but the decoder should separately translate then into corresponding language.
	\item Adversarial training
	Train discriminator to classify between the encoding of source and target sentences. The discriminator operates on the output of the encoder, the encoder is trained instead to fool the discriminator. 
\end{itemize}


\textbf{Optimization}\\
When minimizing the loss function, gradients will not be back propagated through the reverse model which generate the data. Instead the objective function minimized at every iteration is the sum of $L^{auto}$ and $L^{back}$
\begin{itemize}
	\item Denoising autoencoder loss
	\[ L^{auto} = \mathbb{E}_{\bm{e}\sim E}[-\log P_{t\rightarrow t}(\bm{e}|\text{noise}(\bm{e})] + \mathbb{E}_{\bm{f}\sim F} [-\log P_{s\rightarrow s}(\bm{f}|\text{noise}(\bm{f}))]\]
	where $s$ $P_{s\rightarrow s}$ and $P_{t\rightarrow t}$ are the composition of encoder and decoder both operating in the source and target sides, respectively
	\item Back-translation loss
	\[ L^{back} = \mathbb{E}_{\bm{e}\sim T} [-\log P_{s\rightarrow t}(\bm{e}|u(\bm{e}))] +  \mathbb{E}_{\bm{f}\sim S} [-\log P_{t\rightarrow s}(\bm{f}|v(\bm{f}))]\]
	 we denote the sentence that translated by intermediate target-to-source translation model as $u(y)$, similarly denote the sentence translated by source-to-target model as $v(x)$, so $u(y)$ should in source language and $v(x)$ should in target language. The pairs $(x, v(x))$, $(u(y), y)$ constitute synthetic parallel  sentences.
\end{itemize}










